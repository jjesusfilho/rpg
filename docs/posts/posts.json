[
  {
    "path": "posts/2021-03-25-colao-no-r-e-no-postgresql/",
    "title": "Colação no R e no PostgreSQL",
    "description": "Neste tutorial, mostro como usar regras de colação para ajustar tanto o R quanto o \nPostgreSQL para ignorar acentos, maiúsculas  e minúsculas, ordenar sequências numéricas contidas em texto e ignorar \npontuação em números, como em CPF ou CNPJ.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-03-25",
    "categories": [],
    "contents": "\nColação\nPossivelmente, você já se encontrou numa situação em que pediu para o R ou o PostgreSQL ordenar uma coluna de texto e ficou surpreso ao ver que ‘20’ apareceu depois de ‘100’ ou não sabe a razão porque caixa baixa vem antes de caixa alta ou porque ‘Ágora’ vem depois de ‘agora’ ou, mais grave, não sabe quem vem primeiro se ‘vovó’ ou ‘vovô’.\nAlém disso, por vezes, você quer realizar buscas ignorando acentuação e caixa. Por exemplo, sabe que o nome do juiz André de Paula pode estar escrito das seguintes formas: André de Paula, Andre de Paula ou ANDRE DE PAULA. Naturalmente, você pode preprocessar os termos de busca e usar expressões regulares para garantir que qualquer das três forma que a pessoa escrever, o resultado será aquele armazenado em sua tabela.\nUma outra situação que você seguramente já se encontrou foi realizar buscas com sequências de números separados por pontuação, mas a sequência na sua tabela não está pontuada ou vice-versa. Bons exemplos são o CNPJ, o CPF, o CEP ou o número do processo judicial.\nA essa altura, você já deve ter perguntado existem regras para ordenar e comparar caracteres. A esse conjunto de regras para comparar e ordenar caracteres, respeitando os diferentes idiomas, chamamos de colação.\n\n\n\nColação deve ser usada com cuidado para não ter surpresas como alguém pedir um bolo de coco e receber um bolo de cocô.\nNeste tutorial, assumimos que tanto o R quanto o PostgreSQl estão configurados para usar unicode, mais especificamente UTF-8. Se você não adotou unicode, melhor fazê-lo para evitar infindáveis problemas com comparação ou mesmo visualização de caracteres.\nAs regras de colação são amplas e à primeira vista um tanto complexas, você pode consultá-las aqui. Neste tutorial trataremos das mais comumente utilizadas, que são aquelas que tratam de caixa, i.e. maiúsculo ou minúsculo, acentuação, ordem numérica e pontuação.\nUnicode e ICU\nUnicode é um padrão de tecnologia da informação para dar consistência ao encoding, à representação e ao manuseio de caracteres wikipedi. Encoding é basicamente um sistema, dentre muitos, adotado para representar caracteres numericamente, na linguagem do computador.\nAntes do Unicode ascender como padrão preferível, havia uma multiplicidade de regras definidas conforme a conveniência da lingua ou do criador do programa. Infelizmente ainda há, mas ao menos agora todos têm uma opção unificadora que cobre praticamente todos os caracteres existentes.\nICU (International Components for Unicode) é um conjunto de bibliotecas escritas para C/C++ e Java para permitir que programas de computador adotem o Unicode. No R, os pacotes stringi e stringr usam ICU não só para colação, como também para expressões regulares. No PostgreSQL, a partir da versão 12 passou a ser possível optar por colação não-determinística e, com isso, aplicar as várias opções de ICU para colação.\nDataframe\nPara iniciar, vamos criar um dataframe que nos permita trabalhar tanto no R quanto no PostgreSQL e aplicar as quatro opções indicadas: caixa, acentuação, pontuação e numérico.\n\n\nlibrary(tidyverse)\ndf <- data.frame(nome = c(\"Mário\",\"Flávia\",\"Angélica\",\"Vinícios\"),\n             cpf = c(\"432.097.759-39\", \"017.372.652-92\", \"612.260.255-65\", \"828.483.096-08\"),\n             codigo = c(\"g123\",\"g27\",\"g34\",\"g257\")\n                )\n\nknitr::kable(df)\n\n\nnome\ncpf\ncodigo\nMário\n432.097.759-39\ng123\nFlávia\n017.372.652-92\ng27\nAngélica\n612.260.255-65\ng34\nVinícios\n828.483.096-08\ng257\n\nColação no R\nO pacote stringi possui funções específicas para aplicar regras de colação. Você controla o comportamento do “colator” ICU levando em conta os quatro problemas acima mencionados por meio de quatro argumentos:\n\n\ntb <-\"\n| argumento          | opções | padrao | descrição                      | \n|--------------------|--------|        |--------------------------------|\n| strength           | 1L     |        | ignora caixa e acentos         |\n|                    | 2L     |        | ignora caixa                   |\n|                    | 3L     |   x    |                                |\n|                    | 4L     |        |                                |\n| alternated_shifted | TRUE   |        | ignora pontuação               |\n|                    | FALSE  |   x    | não ignora pontuação           |\n| numeric            | TRUE   |        | considera sequência e números  |\n|                    | FALSE  |   x    | considera cada número          |\n| locale             | NULL   |   x    | Não define locale              |\n|                    | ''     |        | Locale padão                   |\n|                    | outros |        | outros locales,e.g.,pt_BR-UTF-8|\n\"\ncat(tb)\n\n\n\n| argumento          | opções | padrao | descrição                      | \n|--------------------|--------|        |--------------------------------|\n| strength           | 1L     |        | ignora caixa e acentos         |\n|                    | 2L     |        | ignora caixa                   |\n|                    | 3L     |   x    |                                |\n|                    | 4L     |        |                                |\n| alternated_shifted | TRUE   |        | ignora pontuação               |\n|                    | FALSE  |   x    | não ignora pontuação           |\n| numeric            | TRUE   |        | considera sequência e números  |\n|                    | FALSE  |   x    | considera cada número          |\n| locale             | NULL   |   x    | Não define locale              |\n|                    | ''     |        | Locale padão                   |\n|                    | outros |        | outros locales,e.g.,pt_BR-UTF-8|\n\nEu particularmente considero as opções de argumento no stringi bem confusas. Se você ler a documentação oficial da ICU, verá que no primeiro nível você desconsidera acentos e caixa, mas para ignorar acentos, mas não caixa, você ajusta Case_Level para ‘On’. Este é o mesmo comportamento no PostgreSQL. Com o pacote stringi, não acontece nada se você optar por case_level = TRUE. Para que isso aconteça, você tem de ajustar o locale para não NULL, ou seja, para ’’ (padrão) ou para algum idioma.\nOrdem numérica\nVamos o dataframe criado acima pela coluna codigo:\n\n\ndf %>% \n  arrange(codigo)\n\n\n      nome            cpf codigo\n1    Mário 432.097.759-39   g123\n2 Vinícios 828.483.096-08   g257\n3   Flávia 017.372.652-92    g27\n4 Angélica 612.260.255-65    g34\n\nPossivelmente não era esse o resultado que você queria, pois esperava que “g123” viesse depois de “g27” e de “g34”.\nSe você quiser garantir a ordenação conforme os número, use a função stri_sort com o argumento numeric = TRUE. Dessa forma, o R usará colação para garantir a ordem desejada:\n\n\nlibrary(stringi)\nstri_sort(df$codigo, numeric = TRUE)\n\n\n[1] \"g27\"  \"g34\"  \"g123\" \"g257\"\n\nNo entanto, para usar dentro do arrange, você deve informar qual posição deve ocupar cada um desses valores. Isto é, que o “g123” vá para a terceira posição, o “g27” vá para a primeira e assim por diante. Para tanto, a função match do pacote base vem ao socorro.\n\n\ndf %>% \n  arrange(match(codigo, stri_sort(codigo, numeric = TRUE)))\n\n\n      nome            cpf codigo\n1   Flávia 017.372.652-92    g27\n2 Angélica 612.260.255-65    g34\n3    Mário 432.097.759-39   g123\n4 Vinícios 828.483.096-08   g257\n\nAcentos e Caixa\nVamos olhar agora para o filtro por nome. Se você quiser filtrar por “Mario” sem acento, você poderia simplesmente remover os acentos antes. Vejamos:\n\n\ndf %>% \n  mutate(nome = stringi::stri_trans_general(nome,\"latin-ascii\")) %>% \n  filter(nome == \"Mario\")\n\n\n   nome            cpf codigo\n1 Mario 432.097.759-39   g123\n\nE se quiser garantir que caso o filtro seja por “mario”, “MARIO” ou “Mario”, poderia recorrer a regex, com a vantagem de que não precisa transformar a variável:\n\n\ndf %>% \n   filter(stri_detect_regex(nome,\"(?i)m[áa]rio\"))\n\n\n   nome            cpf codigo\n1 Mário 432.097.759-39   g123\n\nAs desvantagens dessas duas abordagens são notórias, pois se sua intenção é realizar a busca conforme um termo fornecido pelo usuário, regex irá ajudar muito pouco. Para essas situações, colação vem a calhar:\n\n\ndf  %>% \n   filter(stri_detect_coll(nome,\"mario\", strength = 1L))\n\n\n   nome            cpf codigo\n1 Mário 432.097.759-39   g123\n\nPara filtrar, eu usei a função stri_detect_coll. Esta função usará regras de colação para comparar os caracteres. A regra usada foi a de nível 1, argumento strength = 1L, ou seja, a mais flexível, indicando que serão ignorados os acentos e a caixa.\nSe você usar o nível 2, apenas caixa será ignorada:\n\n\nstri_detect_coll(\"mário\",\"MÁRIO\", strength = 2L)\n\n\n[1] TRUE\n\nPorém, obterá FALSE se não acentuar a palavra.\n\n\nstri_detect_coll(\"mário\",\"MARIO\", strength = 2L)\n\n\n[1] FALSE\n\nSe quiser ignorar a acentuação, mas não ignorar a caixa, terá de manter o nível 1 e informar o locale padrão:\n\n\nstri_detect_coll(\"mário\",\"mario\", strength = 1L, locale = '')\n\n\n[1] TRUE\n\nAgora vamos manter o mesmo nível 1 e alterar a caixa:\n\n\nstri_detect_coll(\"mário\",\"Mário\", strength = 1L, locale = '')\n\n\n[1] FALSE\n\nComo já disse acima, esse não é o comportamento esperado e me parece confuso, mas funciona.\nIgnorar pontuação\nPor fim, podemos olhar para a coluna cpf. Ela foi armazenada com os números separados por ponto ou hífen. Se quisermos que o usuário eventualmente forneça números para filtrar sem dígito, ajustamos o valor do argumento alternate_shifted para TRUE, de modo a ignorar qualquer pontuação. Vejamos:\n\n\ndf %>% \n  filter(stri_detect_coll(cpf,\"43209775939\", alternate_shifted = TRUE))\n\n\n   nome            cpf codigo\n1 Mário 432.097.759-39   g123\n\nColação no PostgreSQL\nNo PostgreSQL, podemos usar as mesmas regras, com a diferença que a sintaxe muda e me parece mais consistente que o R. Vale notar que, por padrão, o PostgreSQL usa colação determinística, ou seja, a comparação é por identidade da sequência de bites. Por exemplo, a \\(\\ne\\) á. Colação não determinística, adicionada na versão 12 do PostgreSQL, considera alguns caracteres como iguais, mesmo quando eles têm sequências diferentes de bites. Por exemplo, a = á = A = Á.\nA vantagem no uso de colação no PostgreSQL é que você pode armazenar os dados de uma forma e flexibilizar para o usuário como ele irá realizar buscas. Por exemplo, você pode armazenar o CNPJ sem pontuação e deixar que o usuário busque com ou sem pontos e vice-versa.\nBasicamente, no PostgreSQL criamos uma colação e, ao criar ou alterar a coluna, indicamos qual colação será usada. Se não indicarmos, será adotada a do locale e determinística.\nInicialmente, vamos criar a colação para a coluna codigo. Se você consultar as opções de ajuste na página oficial da ICU, verá que para ordem numérica, usa-se o parâmetro kn-on.\nCREATE COLLATION colacao_num (\nprovider = 'icu',\nlocale = 'und-u-kn-on'\ndeterministic = false\n);\nPrimeiramente, demos o nome colacao_num, que será usado mais adiante para alterar a colação da coluna codigo. Depois definimos o provedor, vez que o PostgreSQL não admite somente a ICU. Em seguida, definimos o locale com a inclusão das regras de colação. und-u nenhuma linguagem específica e mantêm o locale padrão. A chave kn quando ligada on considera o valor numérico da sequência de números. Além disso, você\nSe você ordenar a tabela pela coluna número, verá que a ordem não é aquela desejada:\nSELECT * FROM df ORDER BY codigo;\n\n   nome   |      cpf       | codigo\n----------+----------------+--------\n Mário    | 432.097.759-39 | g123\n Vinícios | 828.483.096-08 | g257\n Flávia   | 017.372.652-92 | g27\n Angélica | 612.260.255-65 | g34\n(4 rows)\n\nVamos alterar a colação da coluna codigo para aquela que acabamos de criar.\n\nalter table df alter column codigo set data type text collate colacao_num;\nO resultado sai como esperado:\nSELECT * FROM df ORDER BY codigo;\n\n\n   nome   |      cpf       | codigo\n----------+----------------+--------\n Flávia   | 017.372.652-92 | g27\n Angélica | 612.260.255-65 | g34\n Mário    | 432.097.759-39 | g123\n Vinícios | 828.483.096-08 | g257\n(4 rows)\n\nIgnorar acentos e caixa\nVamos agora criar uma colação para ignorar acentos e caixa a coluna nome. Para tanto, basta você optar para level1 na chave ks:\nCREATE COLLATION  colacao_nome (\n\nprovider = 'icu',\nlocale = 'und-u-ks-level1',\ndeterministic = 'false'\n\n);\nAltere a colação:\nALTER TABLE df ALTER COLUMN nome  SET DATA TYPE text COLLATE colacao_nome;\nAgora vamos filtrar para ‘angelica’, ou seja, tudo em minúsculo e sem acento:\nSELECT * FROM df WHERE nome = 'angelica';\n\n\n   nome   |      cpf       | codigo\n----------+----------------+--------\n Angélica | 612.260.255-65 | g34\n(1 row)\n\nIgnorar pontuação\nPor fim, vamos ajustar para ignorar pontuação na coluna cpf. Para tanto, você precisa ajustar a chave ‘ka’ para ‘shifted’:\nCREATE COLLATION colacao_pontuacao (\nprovider = 'icu',\nlocale = 'und-u-ka-shifted',\ndeterministic = 'false';\n\n);\nAlterando a coluna cpf para aceitar a nova colação criada:\nALTER TABLE df ALTER COLUMN cpf SET DATA TYPE text collate colacao_pontuacao;\nVamos filtrar para o cpf da Flávia sem incluir pontos:\nSELECT * FROM df WHERE cpf = '01737265292';\nE o resultado:\n\n\nd <- \"\n  nome  |      cpf       | codigo\n--------+----------------+--------\n Flávia | 017.372.652-92 | g27\n(1 row)\n\"\ncat(d)\n\n\n\n  nome  |      cpf       | codigo\n--------+----------------+--------\n Flávia | 017.372.652-92 | g27\n(1 row)\n\nConsiderações finais\nColação é um recurso fantástico para realizar buscas e ordenar tabelas com bastante flexibilidade, sem ter de promover alterações nas colunas.\n\n\n\n",
    "preview": "posts/2021-03-25-colao-no-r-e-no-postgresql/colacao.png",
    "last_modified": "2021-03-25T08:59:21-03:00",
    "input_file": "colao-no-r-e-no-postgresql.utf8.md"
  },
  {
    "path": "posts/2021-01-05-trabalhando-com-mapas-com-r-e-postgresqlpostgis/",
    "title": "Trabalhando com mapas com R e PostgreSQL/PostGIS",
    "description": "Neste tutorial, irei mostrar como armazenar shapes no PostgreSQL\nusando a extensão PostGIS e criar mapas no R sem sobrecarregar a memória.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-05",
    "categories": [],
    "contents": "\nIntrodução\nNesta postagem, irei mostrar como usar a extensão PostGIS para armazenar shapes no formato simple features (sf) e chamá-las do R. O R possui um importante pacote chamdo sf que, assim como o PostGIS usa o padrão simple feature para organizar os shapes. Esta similaridade apresenta várias vantagens, sendo que a principal delas é a comunicação sem percalços entre o R e o PostgreSQL, quando se trata de trabalhar com mapas.\nAlém disso, muitas das funções tanto da extensão PostGIS quanto do pacote sf têm nomes similares, começando com st_. Por fim, o pacote sf já possui algumas funções para escrever e ler polígonos do PostgreSQL, o que facilita em muito em termos de compatibilidade dos tipos.\nHá muitas vantagens em armazenar shapes no PostgreSQL. Geralmente, shapes ocupam muito espaço na memória. Se eles estão todos no banco, os problemas praticamente se acabam. Você pode criar queries para filtrar somente o que você quer, juntar tabelas etc. Seu shinyapp vai ficar leve como uma pena. Bastará montar um query parametrizada com o uso da função glue_sql do pacote glue, que os usuários do seu shiny poderão plotar uma infinidade de mapas, sem que a memória seja sobrecarregada.\nPrérrequisitos no PostgreSQL\nPrimeiramente, devemos instalar o PostGIS no servidor onde se encontra o PostgreSQL. Eu estou usando o PostgreSQL versão 12 e irei instalar nele a versão 3 do PostGIS.\nsudo apt install postgis postgresql-12-postgis-3\nFeito isso, vamos entrar no PostgreSQL:\nsudo -u postgres psql\nVamos criar um banco de dados, onde instalaremos a extensão PostGIS. Estou assumindo que você já criou um role/usuário com o qual costuma conectar-se do R. O meu é jose e ele será o proprietário do banco criado. Eu chamarei o banco de geobr em homenagem ao pacote geobr.\ncreate database geobr owner jose;\nAgora, vamos nos connectar a banco recentemente criado.\n\\c geobr jose\nFeito isso, podemos criar a extensão PostGIS:\ncreate extension postgis;\nPrérrequisitos no R\nNo servidor onde se encontra o R, precisamos instalar as dependências para então instalar os pacotes geobr e sf. A instalação do geobr também instala o sf. Vá para o terminal e proceda da seguinte forma:\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt update\nsudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev\nFeito isso, basta instalar o pacote geobr diretamente do console do R:\ninstall.packages(\"geobr\")\nTrabalhando com o PostGIS a partir do R\nAgora você verá como é simples e mágico trabalhar mapas no R sem se preocupar a memória. Vamos enviar o shape do mapa do Brasil para os PostgreSQL.\nlibrary(geobr)\nlibrary(sf)\nbr <- read_country(year = 2019, simplified = TRUE, showProgress = TRUE)\nConnecte-se ao PostgreSQL:\nconn <- DBI::dbConnect(RPostgres::Postgres(), host= \"endereco_servidor_postgres\", dbname = \"geobr\", user = \"seu_role\", password = \"sua_senha\")\nAgora é só enviar o mapa pra lá, usando a função st_write do pacote sf. Simples, não?\nst_write(br, conn)\nPreechimento\nTemos apenas o shape, precisamos preencher esses shapes com dados. Para tanto, usarei o pacote brcities que extrai informações populacionais do IBGE. Se quiser saber quais informações populacionais você pode baixar com este pacote, pode consultá-lo aqui. Se quiser instalá-lo, use o seguinte cógido:\nremotes::install_github(\"abjur/brcities\")\nVamos carregá-lo, bem como, o tidyverse e o sf, para limpar e organizar os dados antes de enviá-los para o PostgreSQL.\nlibrary(brcities)\nlibrary(tidyverse)\nlibrary(sf)\nUsaremos a função br_city_indicators(). Esta função baixa dados municipais de uma unidade federativa por vez. Usaremos a função map_dfr do pacote purrr para baixar de todas e já empilhá-las num único dataframe.\nsiglas <- c(\"RO\", \"AC\", \"AM\", \"RR\", \"PA\", \"AP\", \"TO\", \"MA\", \"PI\", \"CE\", \n\"RN\", \"PB\", \"PE\", \"AL\", \"SE\", \"BA\", \"MG\", \"ES\", \"RJ\", \"SP\", \"PR\", \n\"SC\", \"RS\", \"MS\", \"MT\", \"GO\", \"DF\")\n\nindicador <- 25207L\nuf_pop <- map_dfr(siglas, ~br_cities_indicator(.x, indicators = indicador))\nAgora, vamos fazer alguns pequenos ajustes no dataframe uf_pop antes de enviá-lo ao PostgreSQL. Iremos remover a última coluna, renomear a oitava coluna que contêm a população para pop, convertê-la para integer e escrevê-la no banco com o nome uf_pob.\n if_pop <-  uf_pop %>% \n      select(-9) %>%\n      rename(pop =8) %>% \n      mutate(pop = as.integer(pop))\n\n      DBI::dbWriteTable(conn,\"uf_pop\",uf_pop)\nPlotando o mapa\nPronto, estamos em condições de montar uma query para trazer os dados de interesse e plotá-los com ggplot2. Irei usar CTE (Common Table Expressions), que nada mais é do que um query temporário, para somar a população dos municípios por uf e depois juntá-las (inner join) no sf chamado br que foi inicialmente escrito no PostgreSQL.\nÉ importante notar que você deve usar a função st_read() do pacote sf. Do contrário, o polígono não será importado como geometry.\n\n\n\n\n\ndados <- st_read(conn, query = \"\n                      with cte_pop as \n                      (select uf, sum(pop) pop\n                      from uf_pop\n                      group by uf\n                      )\n                      select br.abbrev_state, br.name_state, cte_pop.pop, br.geom\n                      from br\n                      inner join cte_pop on cte_pop.uf = br.abbrev_state\n                      \")\n\n\n\nAgora é ir para o abraço e plotar o mapa.\n\n\ndados %>% \n  mutate(pop = (pop/1000) %>% round(0)) %>% \nggplot() +\n  geom_sf(aes(fill = abbrev_state), show.legend = FALSE)+\n  geom_sf_label(aes(label = pop),size = 2)+\n  scale_fill_viridis_d()+\n  theme_bw()\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-05-trabalhando-com-mapas-com-r-e-postgresqlpostgis/trabalhando-com-mapas-com-r-e-postgresqlpostgis_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-01-05T13:04:33-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-busca-textual-com-r-shiny-e-postgresql/",
    "title": "Busca textual com R, Shiny e PostgreSQL",
    "description": "Este tutorial mostra como configurar busca textual num esquema do\nPostgreSQL e montar um aplicativo shiny para realizar buscas eficientes",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\nIntrodução\nHá alguns anos, eu tenho me dedicado a aperfeiçoar ferramentas de coleta, limpeza, organização e análise de dados processuais. Posso afirmar, com segurança, que tenho bem elaborado um processo que dá conta eficientemente de todo o ciclo da ciência de dados utilizando apenas dois softwares livres: R e PostgreSQL.\nEste tutorial mostrará como estruturar uma base de dados de textos no PostgreSQL, tokenizá-los e montar um índice invertido a fim de realizar buscas textuais em grande volumes de documentos em poucos segundos. Os textos serão organizados a partir do R, enviados para o PostgreSQL, indexados e, de lá,́ chamados a partir de um aplicativo shiny.\nBusca textual\nA busca textual confere a capacidade de identificar documentos em linguagem natural que atendam a uma consulta e, opcionalmente, classificá-los por relevância para a busca. O tipo mais comum de pesquisa é encontrar todos os documentos que contenham os termos da consulta e retorná-los em ordem de semelhança com a consulta. As noções de consulta e semelhança são muito flexíveis e dependem da aplicação específica.\nOs operadores de pesquisa textual existem nos bancos de dados há anos. O PostgreSQL possui operadores ~, ~ *, LIKE e ILIKE para tipos de dados textuais, mas eles não possuem muitas propriedades essenciais exigidas pelos modernos sistemas de informação:\nNão há suporte linguístico, mesmo para o inglês. Expressões regulares não são suficientes porque não conseguem lidar facilmente com palavras derivadas, por exemplo, satisfazer e satisfeito.\nEles não ordenam (classificação) os resultados da pesquisa conforme a relevância, o que os torna ineficazes quando milhares de documentos correspondentes são encontrados.\nEles tendem a ser lentos porque não há suporte de índice; portanto, eles devem processar todos os documentos para cada pesquisa.\nEm outras palavras, para uma busca textual eficiente, é importante tomar em consideração ferramentas de NLP e um pré-processamente dos textos.\nA indexação de texto permite que os documentos sejam pré-processados e um índice salvo para posterior busca rápida. O pré-processamento inclui:\nTokenização dos documentos;\nConversão dos tokens em lexemas;\nSalvar documentos pré-processados e otimizados para pesquisa;\nPorque usar o R\nSoftware live;\nAcolhedor da diversidade;\nComposto por uma comunidade acadêmica exigente e cientificamente rigorosa;\nPossui um grande número de pacotes para coleta, limpeza e estruturação de dados. É importante lembrar que esse trabalho chega a tomar 80% do ciclo de ciência de dados;\nDá conta tanto de estatística quanto de machine learning;\nFacilita a publicação de resultados tanto com relatórios (Rmarkdown) quanto com aplicativos (shiny);\nPorque usar o PostgreSQL\nSoftware livre;\nFácil de instalar;\nBem documentado;\nAmpla comunidade;\nFunciona como motor de busca textual, dispensando o uso do Solr ou do Elasticsearch;\nAssumindo que você já tem o R, o RStudio, e o shiny-server instalados, irei mostar apenas como instalar o PostgreSQL. Caso queira instalar os três primeiros, você pode seguir este script para instalá-los no Ubuntu.\nInstalação do PostgreSQL\nSupondo um ambiente de desenvolvimento, irei considerar a instalação do PostgreSQL, do RStudio e do Shiny numa única máquina. Em produção, eu criaria uma rede privada de máquinas virtuais e distribuiria as funcionalidades em diferentes máquinas numa mesma central de dados (data-center).\n- PostgreSQL instalado (irei mostrar como instalar no Ubuntu);\n- R, RStudio e Shiny instalados;\n- Pacotes RPostgres, dbx, pool, glue,abjutils e DT instalados;\n- Tidyverse \nInstalar o PostgreSQL\nO procedimento abaixo mostra como instalar o PostgreSQL\nAdicionar a chave GPG\nA instalação da chave GPG preserva uma comunicação segura entre o cliente eo servidor. Ela é importante para assegurar a integridade dos dados e a autencidade da fonte. Ou seja, os dados são criptografados antes de serem baixados por seu computador e decriptografados pela chave previamente instalada. Isso reduz significativamente as chances de que um terceiro intervenha no processo de transmissão e instale algo nocivo na sua máquina.\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nEm seguida, adicione o repositório com o comando abaixo:\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list'\nFeito isso, o passo seguinte é instalar o PostgreSQL:\nsudo apt update\nsudo apt install postgresql postgresql-contrib\nConfiguração do locale\nO PostgreSQL adota o locale da sua máquina, então é importante assegurar que o locale está configurado para pt_BR.UTF-8. Crie um arquivo e adicione o script abaixo para configuração do locale:\nsudo touch set_locale.sh ## criação do arquivo\nScript:\n#!/bin/bash\n# Set locales in /etc/default/locale file\necho \"Setting locale...\"\necho \"# Locale settings\nexport LANGUAGE=pt_BR.UTF-8\nexport LANG=pt_BR.UTF-8\nexport LC_ALL=pt_BR.UTF-8\" >> ~/.bash_profile\nlocale-gen pt_BR.UTF-8\nsudo dpkg-reconfigure locales\nsource ~/.bash_profile\nsudo chmod +x set_locale.sh\nsudo ./set_locale.sh\nTrabalhando com o PostgreSQL\nPara fins de completude, estou admitindo que você não tem familiaridade com o PostgreSQL. Isso não significa que darei explicação de cada passo, mas apenas que não os deixarei implícitos.\nHá muitos clientes que permitem acesso ao PostgreSQL para envio de queries e statements. Nós usaremos dois, o psql e o próprio R. Com o psql você acessa e trabalha com o Posgres via linha de comando. Quando você instalou o PostgreSQL, o psql também foi instadado. Dito isso, vamos realizar nosso primeiro acesso.\nsudo -u postgres psql\nTO_TSVECTOR, TO_TSQUERY e @@\nAs funções to_tsvector, to_tsquery e o operador @@ (match) fazem a mágica da busca textual.\nSELECT to_tsvector('portuguese',\n'Alma minha gentil, que te partiste\nTão cedo desta vida descontente,\nRepousa lá no Céu eternamente,\nE viva eu cá na terra sempre triste.(Camões)') @@ to_tsquery('céu');\n\n?column?\n----------\n t\n(1 row)\nSELECT to_tsvector('portuguese',\n'Minha mãe me deu ao mundo\ne, sem ter mais o que me dar,\n\nme ensinou a jogar palavra\nno vento pra ela voar.\n\nDizia: “Filho, palavra\nTem que saber como usar.\n\nAquilo é que nem remédio:\n\nCura, mas pode matar.(Aleixo)') \n@@ to_tsquery('filho & remédio');\n?column?\n----------\n t\n(1 row)\nSELECT to_tsvector('portuguese',\n'No fundo, no fundo,\nbem lá no fundo,\na gente gostaria\nde ver nossos problemas\nresolvidos por decreto\n\na partir desta data,\naquela mágoa sem remédio\né considerada nula\ne sobre ela — silêncio perpétuo\n\nextinto por lei todo o remorso,\nmaldito seja quem olhar pra trás,\nlá pra trás não há nada,\ne nada mais\n\nmas problemas não se resolvem,\nproblemas têm família grande,\ne aos domingos\nsaem todos a passear\no problema, sua senhora\ne outros pequenos probleminhas.(Leminski)') \n@@ to_tsquery('remorso | probleminhas');\n?column?\n----------\n t\n(1 row)\nTrabalhando com tabelas\nNo seguinte repositório consta uma base de 48 mil notícias do G1, a qual utilizaremos para fins de demonstração.\nInicialmente, vamos criar um usuário (role) e uma base de dados para receber essas notícias:\nCREATE ROLE saturday WITH PASSWORD 'RshinesWithPostgres'; \nCREATE DATABASE noticias OWNER = saturday;\nAgora nos conectamos à base, adicionamos a extensão unaccent retirar acentos das palavras.\n\\c noticias\nCREATE EXTENSION unaccent;\nConfigurações necessárias\nVamos agora configurar a busca para que ela lide adequadamente com palavras acentuadas, maiúsculas e minúsculas, bem como, de suas variações.\nCREATE TEXT SEARCH CONFIGURATION pt (COPY = pg_catalog.portuguese);\nALTER TEXT SEARCH CONFIGURATION pt\nALTER MAPPING\nFOR hword, hword_part, word with unaccent, portuguese_stem;\nIndexação dos documentos\nDe agora em diante, passaremos a executar os queries e statemants a partir do próprio R, colocando-os dentro de funções.\nA primeira coisa a fazer é conectar-se à base e adicionar a tabela. Veja que eu apenas crio a tabela, mas não insiro os documentos. Quando você tem muitos documentos, isso pode travar.\nconn <- DBI::dbConnect(RPostgres::Postgres(),\ndbname = \"noticias\",\nhost = \"localhost\",\nuser=\"saturday\",\npassword = \"RshinesWithPostgres\")\n\nDBI::dbCreateTable(conn,\"g1\",g1)\nInserindo os documentos\nPara inserir os documentos, eu prefiro usar o pacote dbx porque ele permite a inserção em batches. Inserir centenas de milhares de documentos pode sobrecarregar sua máquina. Coloquei mil, mas 50 mil tem suportado bem.\ndbx::dbxInsert(con = conn, table = \"g1\", records = g1, batch_size = 1000)\nIndexando os documentos\nHora de indexar os documentos. Há dois indexadores, o GIN e o GIST, usaremos o GIN pq é mais rápido, porém mais intenso. A função a seguir cria o index estabelecendo pesos diferentes para duas colunas.\npsql_tokenize <- function(con, tbl, config = \"pt\") {\n  source <- list(a = c(\"intro\", \"A\"), j = c(\"corpo\", \"B\"))\n  target <- \"document_tokens\"\n  idx <- paste0(tbl,\"_idx\")\n  query <- glue::glue_sql(\"ALTER TABLE {`tbl`} ADD COLUMN {`target`} TSVECTOR\", .con = con)\n\n  res <- DBI::dbSendQuery(con, query)\n  DBI::dbClearResult(res)\n\n  query <- glue::glue_sql(\"UPDATE {`tbl`} SET\n                         {`target`} = setweight(to_tsvector({config},coalesce({`source$a[1]`},'')), {source$a[2]}) ||\n                         setweight(to_tsvector({config},coalesce({`source$j[1]`}, '')), {source$j[2]})\", .con = con)\n\n  res <- DBI::dbSendQuery(con, query)\n  DBI::dbClearResult(res)\n\n  query <- glue::glue_sql(\"CREATE INDEX {`idx`} ON {`tbl`} USING GIN ({`target`})\", .con = con)\n\n  res <- DBI::dbSendQuery(con, query)\n  DBI::dbClearResult(res)\n}\nCriando gatilho (trigger)\nA função a seguir cria um gatilho para indexar novos documentos inseridos:\npsql_trigger <- function(con,tbl,config=\"pt\"){\n\n  a<-\"A\"\n  b<-\"B\"\n  intro<-\"new.intro\"\n  corpo=\"new.corpo\"\n  f_name<-paste0(tbl,\"_trigger()\")\n\n  q<-glue::glue_sql(\"CREATE FUNCTION {DBI::SQL(f_name)} RETURNS trigger AS $$\nbegin\n  new.document_tokens :=\n     setweight(to_tsvector({config},coalesce({intro},'')), {a}) ||\n     setweight(to_tsvector({config},coalesce({corpo},'')), {b});\n  return new;\nend\n$$ LANGUAGE plpgsql;\",.con=con)\n\n  RPostgres::dbExecute(con,q)\n\n  q <- glue::glue_sql(\"\nCREATE TRIGGER tsvectorupdate BEFORE INSERT OR UPDATE\n    ON {`tbl`} FOR EACH ROW EXECUTE FUNCTION {DBI::SQL(f_name)}\",.con=con)\n\n  RPostgres::dbExecute(con,q)\n}\nRealizando buscas\nPor fim, montamos a função para realizar as buscas\npsql_query <-\n  function (con,\n            tbl,\n            query = \"\")\n  {\n    \n    target <- \"document_tokens\"\n    q <-\n      glue::glue_sql(\n        \"SELECT * FROM {`tbl`}  WHERE {`tbl`}.{`target`} @@ websearch_to_tsquery('pt',{query})\",\n        .con = con\n      )\n    DBI::dbGetQuery(con, q)\n  }\nInclusão no aplicativo Shiny\nO repositório FullTextSearch contém template de aplicativo para realizar as buscas.\nIncluí uma função psql_g1_dt.R para criar um datatable htmlwidget com ajustes na aparência.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-04T19:30:48-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-chamando-o-r-do-postgresql/",
    "title": "Chamando o R do PostgreSQL",
    "description": "Como instalar os dois e usá-los de forma integrada.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\nIntrodução\nEste tutorial irá mostrar como instalar o R e o PostgreSQL num mesmo servidor Ubuntu 20.04 ou 18.04. Em seguida, falaremos sobre como chamar o R a partir do PostgreSQL.\nFarei uma série de tutoriais. Este primeiro é voltado para aqueles com familiaridade com funções do R e que gostariam de rodá-las no PostgreSQL, sem necessariamente conhecer muito de SQL. Num próximo, iremos mostrar como criar queries e declarações do PostgreSQL do R, ou seja, será mais voltado para quem tem familiaridade com o SQL, mas não necessariamente versada em R.\nOs tutoriais posteriores serão voltados para aqueles com bastante familiaridade em tidyverse e que gostariam de realizar as mesmas coisas em SQL. Igualmente, servirão para aqueles que sabem manipular dados em SQL, mas gostariam de fazer as mesmas coisas no R.\nEm futuros tutoriais, falaremos quando compensa iniciar no PostgreSQL e terminar no R. Por exemplo, quando é mais vantajoso dar um join no R em vez de fazê-lo no PostgreSQL e vice-versa.\nInstalando o PostgreSQL\nA primeira coisa a fazer é atualizar os pacotes do sistema:\nsudo apt update\nsudo apt -y install vim bash-completion wget\nsudo apt -y upgrade\n\n\n### Configuração do locale\n\nAlém disso, é importante configurar o locale. Crie um arquivo chamado set_locale.sh:\n\n```sh\n$ vim set_locale.sh\nE cole o seguinte conteúdo:\n#!/bin/bash\n\n# Set locales in /etc/default/locale file\necho \"Ajustando o locale...\"\necho \"# Locale settings\nexport LANGUAGE=pt_BR.UTF-8\nexport LANG=pt_BR.UTF-8\nexport LC_ALL=pt_BR.UTF-8\">>~/.bash_profile\n\nlocale-gen pt_BR.UTF-8\n\nsudo dpkg-reconfigure locales\n\nsource ~/.bash_profile\nautorize execução do arquivo:\n$ sudo chmod +x set_locale.sh\nE rode o script:\n$ sudo ./set_locale.sh\nReinicie a máquina:\n$ sudo reboot\nAcesse novavamente via ssh e verifique se o locale foi configurado para pt_BR-UTF8.\n$ locale\nO console deverá imprimir a seguinte configuração:\nLANG=pt_BR.UTF-8\nLANGUAGE=pt_BR.UTF-8\nLC_CTYPE=\"pt_BR.UTF-8\"\nLC_NUMERIC=\"pt_BR.UTF-8\"\nLC_TIME=\"pt_BR.UTF-8\"\nLC_COLLATE=\"pt_BR.UTF-8\"\nLC_MONETARY=\"pt_BR.UTF-8\"\nLC_MESSAGES=\"pt_BR.UTF-8\"\nLC_PAPER=\"pt_BR.UTF-8\"\nLC_NAME=\"pt_BR.UTF-8\"\nLC_ADDRESS=\"pt_BR.UTF-8\"\nLC_TELEPHONE=\"pt_BR.UTF-8\"\nLC_MEASUREMENT=\"pt_BR.UTF-8\"\nLC_IDENTIFICATION=\"pt_BR.UTF-8\"\nLC_ALL=pt_BR.UTF-8\nComo última etapa, configure também o fuso horário:\n$ sudo timedatectl set-timezone America/Sao_Paulo\nConfirme que o fuso horário foi configurado para São Paulo:\n$ timedatectl\nVocê deverá visualizar o seguinte resultado:\nLocal time: Qua 2020-06-17 19:34:30 -03\n  Universal time: Qua 2020-06-17 22:34:30 UTC\n        RTC time: Qua 2020-06-17 22:34:30\n       Time zone: America/Sao_Paulo (-03, -0300)\n Network time on: yes\nNTP synchronized: yes\n RTC in local TZ: no\nImporte a chave GPG:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nAdicione a chave GPG ao systema:\necho \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" |sudo tee  /etc/apt/sources.list.d/pgdg.list\nAtualize os pacotes do sistema e instale os pacotes do PostgreSQL necessários. Os dois primeiros são necessários para rodar o PostgreSQL, os três últimos são necessários para instalar o plr, o RPostgres e outras extensões. Falaremos deles mais adiante.\nsudo apt update\nsudo apt install postgresql-12 postgresql-client-12 postgresql-server-dev-all libpq-dev postgresql-contrib\nInstalação do R\nsudo echo \"deb https://cloud.r-project.org/bin/linux/ubuntu `lsb_release -cs`-cran40/\" | sudo tee -a /etc/apt/sources.list\nAdicionar a chave GPG\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\nInstale o R\nsudo apt-get update\nsudo apt-get install -y r-base r-base-dev \nInstale também o git\nsudo apt install git\nAgora use o git para clonar o plr. Não importa o local onde você irá cloná-lo. Desde que você tenha seguido fielmente os passos anteriores, tudo terminará bem.\ngit clone https://github.com/postgres-plr/plr\nFeito isso, entre no diretório plr e rode os seguintes comandos para instalá-lo como extensão.\ncd plr\nUSE_PGXS=1 make\nUSE_PGXS=1 make install\nCriando uma base de dados\nVamos para o Postgres a fim de criar uma base de dados.\nsudo -u postgres psql\nCREATE DATABASE datasets;\nAlém disso, você deve incluir a extensão plr na base de dados recentemente criada:\n\\c datasets -- conectar-se à base\nCREATE EXTENSION plr;\n\\q -- sair do psql\nDe volta ao R.\nR\nInstale os pacotes RPostgres e broom\nInstalando esses dois pacotes é suficiente para instalar também outras dependências como o DBI e o dplyr, as quais igualmente usaremos.\ninstall.packages(c(\"RPostgres\",\"broom\"))\nConnexão do R ao Postgres\nEventualmente, você terá de autorizar a conexão local. Vá para o arquivo:\nvim /etc/postgresql/12/main/pg_hba.conf\nE altere a seguinte linha de:\nlocal all all peer\nPara:\nlocal all all trust\nColocando uma tabela na base de dados\nAdmitindo que você ainda se encontra no R, estamos em condições de incluir uma tabela na base de dados.\nPrimeiramente, vamos conectar-nos à base:\nconn <- DBI::dbConnect(RPostgres::Postgres(), dbname=\"datasets\")\nEstou admidindo com o código acima que você está usando o R na mesma máquina do Postgres, usando o usuário postgres e dispensou o uso de senha para conexão local.\nVamos enviar o dataframe mtcars para a base de dados. O exemplo do mtcars não é muito feliz porque ele poderia ser chamado do próprio R quando rodado no Postgres, mas apenas a título de exemplo, iremos assumir que ele seja qualquer outro data.frame.\nDBI::dbWriteTable(conn,\"tabela\", mtcars)\nDe volta ao Postgres\nMostraremos num próximo tutorial como realizar os procedimentos a seguir sem sair do R, mas o propósito deste tutorial é justamente ilustrar como podemos chamar o R do Postgres. Assim, faremos tudo no Postgres mesmo.\nVoltando para o shell, vamos conectar-nos à base datasets:\nsudo -u postgres psql datasets\nVerifique se a tabela chamada “tabela” se encontra na base de dados:\n\\d+ -- ou \n\\d+ tabela\nPreparando o terreno\nVamos criar uma tabela que servirá de referência para receber os resultados de uma regressão linear. Veja que as colunas são as mesmas do tibble retornado pela função tidy do pacote broom, com a diferença de que os pontos foram substituídos pelo sublinhado e os nomes das colunas passados para o português.\ncreate table modelo (termo text, estimativa float8, erro_padrao float8, estatistica float8, p_valor float8);\nCriando uma função plr\nEnfim estamos em condições de criar uma função no PostgreSQL que chama o R para rodar uma regressão linear em uma tabela contida no próprio Postgres:\nCREATE OR REPLACE FUNCTION lm_teste() RETURNS SETOF modelo AS\n$$ \nbase <<- pg.spi.exec('select mpg, wt, qsec, am  from tabela')\ndf <- lm(mpg ~ wt + qsec + factor(am), data=base)\ndf <- broom::tidy(df)\nnames(df) <- c('termo','estimativa','erro_padrao','estatistica','p_valor')\ndf <- dplyr::mutate_at(df,dplyr::vars(2:5), ~round(.,2))\nreturn(df)\n$$\nlanguage 'plr';\nNote que o esqueleto da função é o mesmo para qualquer outra função do PostgreSQL. A diferença é que, para importar um objeto da base de dados para nossa função, devemos usar a função pg.spi.exec.\nRodando a regressão linear\nAgora ficou fácil. Basta chamar a função e ver os resultados:\nselect * from lm_teste();\ndatasets=# select * from lm_teste();\n    termo    | estimativa | erro_padrao | estatistica | p_valor\n-------------+------------+-------------+-------------+---------\n (Intercept) |       9.62 |        6.96 |        1.38 |    0.18\n wt          |      -3.92 |        0.71 |       -5.51 |       0\n qsec        |       1.23 |        0.29 |        4.25 |       0\n factor(am)1 |       2.94 |        1.41 |        2.08 |    0.05\n(4 rows)\nVantagens\nEu apontaria duas principais vantagens em usar o plr:\n1 - Uma vez que a base se encontra no PostgreSQL, você não precisa mais transferi-la para o R a fim de rodar o modelo e retornar o resultado ao PostgreSQL. Essa viagem dos dados torna-se dispensável. No exemplo mostrado, porém, a base irá para uma sessão do R de qualquer forma. Veremos como solucionar isso em tutoriais futuros.\n2 - Você pode continuar trabalhando no R, enquanto seu modelo roda no PostgreSQL. Se o modelo tomar horas, este se torna um problema menor.\n\n\n\n",
    "preview": "posts/2021-01-04-chamando-o-r-do-postgresql/linear_plot.png",
    "last_modified": "2021-01-04T21:37:48-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-como-subir-mltiplos-e-grandes-arquivos-para-o-postgresql-com-r/",
    "title": "Como subir múltiplos e grandes arquivos para o PostgreSQL com R",
    "description": "Neste tutorial, irei mostrar como copiar para uma base de dados\ndo PostgreSQL múltiplos arquivos com que não cabem na memória RAM,\na partir do R, já tradados, sem sobrecarregar a memória.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\nNeste tutorial, irei mostrar como copiar para uma base de dados do PostgreSQL múltiplos arquivos com que não cabem na memória RAM, a partir do R, sem sobrecarregar a memória. Além disso, você poderá realizar todas as transformações necessárias nos dados, antes de inseri-los.\nEu diria que há algumas vantagens nesta solução. A primeira delas é a possibilidade de inspecionar os arquivos antes de inseri-los na base de dados SQL. A segunda é inserir os arquivos em parcelas, sem ter de carregar na memória todas as linhas, principalmente quando esta é formada por milhões de linhas. A terceira é a possibilidade de realizar ajustes prévios, tais como selecionar apenas algumas colunas, manter unicamente linhas de interesse, ajustar encoding e realizar todo tipo de transformação nos dados antes de inseri-los.\nA título de exemplo, iremos utilizar os arquivos da RAIS (relação anual de informações sociais) do Ministério do Trabalho do ano de 2018.\nVocê pode baixar esses dados deste endereço. Eles estão compactados em formato 7z. Para baixá-los, basta usar a função download.file() em conjunção com a função walk do pacote purrr.\nAntes, porém, vamos carregar os pacotes tidyverse, vez que usaremos vários deles. Além disso, para extrair os arquivos compactados, usaremos o pacote archive. Para inspecionar o arquivo, usaremos a função fread do pacote data.table. Para ajustar os nomes das colunas, usaremos o pacote janitor. Os pacotes DBI e dbx serão usados paa criar a tabela no PostgreSQL e inseri-los, respectivamente.\ndevtools::install_github(\"jimhester/archive\")\nlibrary(archive)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(janitor)\nlibrary(DBI)\nlibrary(dbx)\n\n\n\nBaixando os dados\nBaixaremos apenas os arquivos correspondentes aos trabalhadores, excluindo o relativo aos estabelecimentos. Estou supondo que você irá baixá-lo no diretório atual e que este está vazio.\narquivos <- readlines(\"ftp://ftp.mtps.gov.br/pdet/microdados/RAIS/2018/\") %>% \n            str_extract(\"RAIS.+\") %>% \n            str_subset(\"ESTAB\",negate = TRUE) %>% \n            str_c(\"ftp://ftp.mtps.gov.br/pdet/microdados/RAIS/2018/\",.)\n\nwalk(arquivos, download.file)\nUma vez descompactados, esses arquivos ocuparão vários gigas de memória. Somente o arquivo de São Paulo, o maior deles, ocupa 8,9 Gb. Vamos descompactá-los:\narquivos1 <- list.files()\n\nwalk(arquivos,archive_extract)\nInspecionando os dados\nAntes de proceder à transferência dos dados, iremos inspecioná-los. Vamos ler as primeiras dez linhas de um dos arquivos e verificar como elas se apresentam. Note que eu coloquei o enconding “Latin-1”. Fiz isso porque havia tentado ler com o default que é “UTF-8” e não foi possível.\ndf <- fread(\"RAIS_VINC_PUB_SP.txt\",nrows=10,encoding=\"Latin-1\")\n\n\n\n\n\nglimpse(df)\n\n\nRows: 10\nColumns: 62\n$ `Bairros SP`              <int> 9999, 9999, 9999, 9999, 654, 9999…\n$ `Bairros Fortaleza`       <chr> \"{ñ class}\", \"{ñ class}\", \"{ñ cla…\n$ `Bairros RJ`              <chr> \"{ñ class}\", \"{ñ class}\", \"{ñ cla…\n$ `Causa Afastamento 1`     <int> 99, 99, 99, 99, 99, 99, 99, 99, 9…\n$ `Causa Afastamento 2`     <int> 99, 99, 99, 99, 99, 99, 99, 99, 9…\n$ `Causa Afastamento 3`     <int> 99, 99, 99, 99, 99, 99, 99, 99, 9…\n$ `Motivo Desligamento`     <int> 11, 11, 12, 21, 11, 11, 11, 11, 1…\n$ `CBO Ocupação 2002`       <int> 717020, 514320, 784205, 622010, 6…\n$ `CNAE 2.0 Classe`         <int> 41204, 46869, 78205, 1610, 91031,…\n$ `CNAE 95 Classe`          <int> 45217, 51594, 74500, 1619, 92533,…\n$ `Distritos SP`            <int> 9999, 9999, 9999, 9999, 67, 9999,…\n$ `Vínculo Ativo 31/12`     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Faixa Etária`            <int> 4, 6, 5, 5, 6, 7, 4, 5, 6, 5\n$ `Faixa Hora Contrat`      <int> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6\n$ `Faixa Remun Dezem (SM)`  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Faixa Remun Média (SM)`  <int> 2, 2, 3, 2, 4, 3, 4, 2, 5, 3\n$ `Faixa Tempo Emprego`     <int> 3, 4, 1, 1, 6, 4, 4, 1, 7, 1\n$ `Escolaridade após 2005`  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ `Qtd Hora Contr`          <int> 44, 44, 44, 44, 44, 44, 44, 44, 4…\n$ Idade                     <int> 27, 44, 30, 35, 43, 59, 29, 30, 4…\n$ `Ind CEI Vinculado`       <int> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0\n$ `Ind Simples`             <int> 0, 1, 0, 0, 1, 0, 0, 1, 1, 0\n$ `Mês Admissão`            <int> 0, 0, 6, 9, 0, 0, 0, 9, 0, 7\n$ `Mês Desligamento`        <int> 1, 5, 7, 10, 6, 5, 5, 10, 3, 10\n$ `Mun Trab`                <int> 0, 0, 354870, 0, 355030, 350950, …\n$ Município                 <int> 350550, 352440, 354780, 350320, 3…\n$ Nacionalidade             <int> 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ `Natureza Jurídica`       <int> 2062, 2062, 2305, 2305, 2135, 206…\n$ `Ind Portador Defic`      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Qtd Dias Afastamento`    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 10\n$ `Raça Cor`                <int> 2, 2, 2, 4, 2, 2, 6, 2, 2, 8\n$ `Regiões Adm DF`          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Vl Remun Dezembro Nom`   <chr> \"0000000000,00\", \"0000000000,00\",…\n$ `Vl Remun Dezembro (SM)`  <chr> \"000000,00\", \"000000,00\", \"000000…\n$ `Vl Remun Média Nom`      <chr> \"0000001174,62\", \"0000001405,21\",…\n$ `Vl Remun Média (SM)`     <chr> \"000001,23\", \"000001,47\", \"000001…\n$ `CNAE 2.0 Subclasse`      <int> 4120400, 4686902, 7820500, 161099…\n$ `Sexo Trabalhador`        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ `Tamanho Estabelecimento` <int> 4, 2, 10, 7, 1, 1, 1, 4, 2, 8\n$ `Tempo Emprego`           <chr> \"11,9\", \"17,1\", \"0,4\", \"1,2\", \"51…\n$ `Tipo Admissão`           <int> 0, 0, 2, 2, 0, 0, 0, 2, 0, 1\n$ `Tipo Estab`              <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ `Tipo Estab.1`            <chr> \"CNPJ\", \"CNPJ\", \"CNPJ\", \"CNPJ\", \"…\n$ `Tipo Defic`              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Tipo Vínculo`            <int> 10, 10, 50, 10, 10, 10, 10, 10, 1…\n$ `IBGE Subsetor`           <int> 15, 17, 19, 25, 21, 15, 15, 19, 3…\n$ `Vl Rem Janeiro CC`       <chr> \"000001174,62\", \"000001272,42\", \"…\n$ `Vl Rem Fevereiro CC`     <chr> \"000000000,00\", \"000001299,48\", \"…\n$ `Vl Rem Março CC`         <chr> \"000000000,00\", \"000001643,73\", \"…\n$ `Vl Rem Abril CC`         <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Maio CC`          <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Junho CC`         <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Julho CC`         <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Agosto CC`        <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Setembro CC`      <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Outubro CC`       <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Vl Rem Novembro CC`      <chr> \"000000000,00\", \"000000000,00\", \"…\n$ `Ano Chegada Brasil`      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Ind Trab Intermitente`   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Ind Trab Parcial`        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ `Tipo Salário`            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ `Vl Salário Contratual`   <chr> \"1.438,31\", \"1.344,87\", \"1.473,61…\n\nVerificamos alguns problemas com este dataframe. O primeiro deles está nos nomes, que não estão num formato amigável. Outro problema é que as colunas referentes à remuneração estão como caractere. Uma breve inspeção permite verificar que são justamente aquelas colunas que começam com “Vl”. Assim, nossa tarefa ajustar os nomes das colunas e alterar para número as colunas que começam com “Vl”.\nTransformando os dados\nPara ajustar os nomes das colunas, usaremos a função clean_names do pacote janitor:\n\n\ndf <- clean_names(df)\nglimpse(df)\n\n\nRows: 10\nColumns: 62\n$ bairros_sp              <int> 9999, 9999, 9999, 9999, 654, 9999, …\n$ bairros_fortaleza       <chr> \"{ñ class}\", \"{ñ class}\", \"{ñ class…\n$ bairros_rj              <chr> \"{ñ class}\", \"{ñ class}\", \"{ñ class…\n$ causa_afastamento_1     <int> 99, 99, 99, 99, 99, 99, 99, 99, 99,…\n$ causa_afastamento_2     <int> 99, 99, 99, 99, 99, 99, 99, 99, 99,…\n$ causa_afastamento_3     <int> 99, 99, 99, 99, 99, 99, 99, 99, 99,…\n$ motivo_desligamento     <int> 11, 11, 12, 21, 11, 11, 11, 11, 11,…\n$ cbo_ocupacao_2002       <int> 717020, 514320, 784205, 622010, 622…\n$ cnae_2_0_classe         <int> 41204, 46869, 78205, 1610, 91031, 4…\n$ cnae_95_classe          <int> 45217, 51594, 74500, 1619, 92533, 4…\n$ distritos_sp            <int> 9999, 9999, 9999, 9999, 67, 9999, 9…\n$ vinculo_ativo_31_12     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ faixa_etaria            <int> 4, 6, 5, 5, 6, 7, 4, 5, 6, 5\n$ faixa_hora_contrat      <int> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6\n$ faixa_remun_dezem_sm    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ faixa_remun_media_sm    <int> 2, 2, 3, 2, 4, 3, 4, 2, 5, 3\n$ faixa_tempo_emprego     <int> 3, 4, 1, 1, 6, 4, 4, 1, 7, 1\n$ escolaridade_apos_2005  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ qtd_hora_contr          <int> 44, 44, 44, 44, 44, 44, 44, 44, 44,…\n$ idade                   <int> 27, 44, 30, 35, 43, 59, 29, 30, 44,…\n$ ind_cei_vinculado       <int> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0\n$ ind_simples             <int> 0, 1, 0, 0, 1, 0, 0, 1, 1, 0\n$ mes_admissao            <int> 0, 0, 6, 9, 0, 0, 0, 9, 0, 7\n$ mes_desligamento        <int> 1, 5, 7, 10, 6, 5, 5, 10, 3, 10\n$ mun_trab                <int> 0, 0, 354870, 0, 355030, 350950, 35…\n$ municipio               <int> 350550, 352440, 354780, 350320, 355…\n$ nacionalidade           <int> 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ natureza_juridica       <int> 2062, 2062, 2305, 2305, 2135, 2062,…\n$ ind_portador_defic      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ qtd_dias_afastamento    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 10\n$ raca_cor                <int> 2, 2, 2, 4, 2, 2, 6, 2, 2, 8\n$ regioes_adm_df          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ vl_remun_dezembro_nom   <chr> \"0000000000,00\", \"0000000000,00\", \"…\n$ vl_remun_dezembro_sm    <chr> \"000000,00\", \"000000,00\", \"000000,0…\n$ vl_remun_media_nom      <chr> \"0000001174,62\", \"0000001405,21\", \"…\n$ vl_remun_media_sm       <chr> \"000001,23\", \"000001,47\", \"000001,5…\n$ cnae_2_0_subclasse      <int> 4120400, 4686902, 7820500, 161099, …\n$ sexo_trabalhador        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ tamanho_estabelecimento <int> 4, 2, 10, 7, 1, 1, 1, 4, 2, 8\n$ tempo_emprego           <chr> \"11,9\", \"17,1\", \"0,4\", \"1,2\", \"51,9…\n$ tipo_admissao           <int> 0, 0, 2, 2, 0, 0, 0, 2, 0, 1\n$ tipo_estab              <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ tipo_estab_1            <chr> \"CNPJ\", \"CNPJ\", \"CNPJ\", \"CNPJ\", \"CN…\n$ tipo_defic              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ tipo_vinculo            <int> 10, 10, 50, 10, 10, 10, 10, 10, 10,…\n$ ibge_subsetor           <int> 15, 17, 19, 25, 21, 15, 15, 19, 3, …\n$ vl_rem_janeiro_cc       <chr> \"000001174,62\", \"000001272,42\", \"00…\n$ vl_rem_fevereiro_cc     <chr> \"000000000,00\", \"000001299,48\", \"00…\n$ vl_rem_marco_cc         <chr> \"000000000,00\", \"000001643,73\", \"00…\n$ vl_rem_abril_cc         <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_maio_cc          <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_junho_cc         <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_julho_cc         <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_agosto_cc        <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_setembro_cc      <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_outubro_cc       <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ vl_rem_novembro_cc      <chr> \"000000000,00\", \"000000000,00\", \"00…\n$ ano_chegada_brasil      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ ind_trab_intermitente   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ ind_trab_parcial        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ tipo_salario            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ vl_salario_contratual   <chr> \"1.438,31\", \"1.344,87\", \"1.473,61\",…\n\nCom isso, podemos converter as colunas iniciadas por “vl” para numéricas:\n\n\ndf <- df %>% \n  mutate(across(starts_with(\"vl\"),~{\n    .x %>% \n    str_replace_all(\"\\\\.\",\"\") %>% ## Remove os pontos\n    str_replace(\",\",\".\") %>%  ## Substitui a vírgula por ponto\n    as.numeric() ## Converte para número\n}))\n\nglimpse(df)\n\n\nRows: 10\nColumns: 62\n$ bairros_sp              <int> 9999, 9999, 9999, 9999, 654, 9999, …\n$ bairros_fortaleza       <chr> \"{ñ class}\", \"{ñ class}\", \"{ñ class…\n$ bairros_rj              <chr> \"{ñ class}\", \"{ñ class}\", \"{ñ class…\n$ causa_afastamento_1     <int> 99, 99, 99, 99, 99, 99, 99, 99, 99,…\n$ causa_afastamento_2     <int> 99, 99, 99, 99, 99, 99, 99, 99, 99,…\n$ causa_afastamento_3     <int> 99, 99, 99, 99, 99, 99, 99, 99, 99,…\n$ motivo_desligamento     <int> 11, 11, 12, 21, 11, 11, 11, 11, 11,…\n$ cbo_ocupacao_2002       <int> 717020, 514320, 784205, 622010, 622…\n$ cnae_2_0_classe         <int> 41204, 46869, 78205, 1610, 91031, 4…\n$ cnae_95_classe          <int> 45217, 51594, 74500, 1619, 92533, 4…\n$ distritos_sp            <int> 9999, 9999, 9999, 9999, 67, 9999, 9…\n$ vinculo_ativo_31_12     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ faixa_etaria            <int> 4, 6, 5, 5, 6, 7, 4, 5, 6, 5\n$ faixa_hora_contrat      <int> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6\n$ faixa_remun_dezem_sm    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ faixa_remun_media_sm    <int> 2, 2, 3, 2, 4, 3, 4, 2, 5, 3\n$ faixa_tempo_emprego     <int> 3, 4, 1, 1, 6, 4, 4, 1, 7, 1\n$ escolaridade_apos_2005  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ qtd_hora_contr          <int> 44, 44, 44, 44, 44, 44, 44, 44, 44,…\n$ idade                   <int> 27, 44, 30, 35, 43, 59, 29, 30, 44,…\n$ ind_cei_vinculado       <int> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0\n$ ind_simples             <int> 0, 1, 0, 0, 1, 0, 0, 1, 1, 0\n$ mes_admissao            <int> 0, 0, 6, 9, 0, 0, 0, 9, 0, 7\n$ mes_desligamento        <int> 1, 5, 7, 10, 6, 5, 5, 10, 3, 10\n$ mun_trab                <int> 0, 0, 354870, 0, 355030, 350950, 35…\n$ municipio               <int> 350550, 352440, 354780, 350320, 355…\n$ nacionalidade           <int> 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ natureza_juridica       <int> 2062, 2062, 2305, 2305, 2135, 2062,…\n$ ind_portador_defic      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ qtd_dias_afastamento    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 10\n$ raca_cor                <int> 2, 2, 2, 4, 2, 2, 6, 2, 2, 8\n$ regioes_adm_df          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ vl_remun_dezembro_nom   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ vl_remun_dezembro_sm    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ vl_remun_media_nom      <dbl> 1174.62, 1405.21, 1473.60, 968.94, …\n$ vl_remun_media_sm       <dbl> 1.23, 1.47, 1.54, 1.01, 2.14, 1.89,…\n$ cnae_2_0_subclasse      <int> 4120400, 4686902, 7820500, 161099, …\n$ sexo_trabalhador        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ tamanho_estabelecimento <int> 4, 2, 10, 7, 1, 1, 1, 4, 2, 8\n$ tempo_emprego           <chr> \"11,9\", \"17,1\", \"0,4\", \"1,2\", \"51,9…\n$ tipo_admissao           <int> 0, 0, 2, 2, 0, 0, 0, 2, 0, 1\n$ tipo_estab              <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ tipo_estab_1            <chr> \"CNPJ\", \"CNPJ\", \"CNPJ\", \"CNPJ\", \"CN…\n$ tipo_defic              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ tipo_vinculo            <int> 10, 10, 50, 10, 10, 10, 10, 10, 10,…\n$ ibge_subsetor           <int> 15, 17, 19, 25, 21, 15, 15, 19, 3, …\n$ vl_rem_janeiro_cc       <dbl> 1174.62, 1272.42, 0.00, 0.00, 20.50…\n$ vl_rem_fevereiro_cc     <dbl> 0.00, 1299.48, 0.00, 0.00, 20.50, 1…\n$ vl_rem_marco_cc         <dbl> 0.00, 1643.73, 0.00, 0.00, 20.50, 1…\n$ vl_rem_abril_cc         <dbl> 0.0, 0.0, 0.0, 0.0, 20.5, 0.0, 0.0,…\n$ vl_rem_maio_cc          <dbl> 0.0, 0.0, 0.0, 0.0, 20.5, 0.0, 0.0,…\n$ vl_rem_junho_cc         <dbl> 0.0, 0.0, 1473.6, 0.0, 20.5, 0.0, 0…\n$ vl_rem_julho_cc         <dbl> 0.00, 0.00, 1473.60, 0.00, 0.00, 0.…\n$ vl_rem_agosto_cc        <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,…\n$ vl_rem_setembro_cc      <dbl> 0.00, 0.00, 0.00, 1118.01, 0.00, 0.…\n$ vl_rem_outubro_cc       <dbl> 0.00, 0.00, 0.00, 819.87, 0.00, 0.0…\n$ vl_rem_novembro_cc      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ ano_chegada_brasil      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ ind_trab_intermitente   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ ind_trab_parcial        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ tipo_salario            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ vl_salario_contratual   <dbl> 1438.31, 1344.87, 1473.61, 11.18, 2…\n\nTransferindo para a base de dados SQL\nRealizar essas transformações num pequeno dataframe foi fácil. O desafio é programar para que o R leia o arquivo em parcelas, realize as transformações e envie para a base de dados. Assumindo que você já sabe se conectar a uma base SQL, vamos dar os passos seguintes.\nA mágica ficará por conta da função walk do pacote purrr e da função read_delim_chunk do pacote readr. Esta última permite que você leia o arquivo em parcelas (chunks) e aplique uma função callback sobre cada um dos chunks. Nesta função callback, você pode incluir todas as transformações que quiser e um comando para inserir os dados na base SQL.\nInicialmente, vamos criar uma tabela na base de dados com o dataframe transformado:\nDBI::dbCreateTable(conn,\"df\",df)\nVamos criar a função callback. Ela irá ajustar os nomes das colunas, converter aquelas iniciadas por “vl” para numéricas e, por fim, inserir os dados na tabela.\nf <- function(x, pos){\n\n`%>%` <- magrittr::`%>%`  \n\nx %>% \njanitor::clean_names() %>% \ndplyr::mutate(dplyr::across(dplyr::starts_with(\"vl\"),~{\n    .x %>% \n    stringr::str_replace_all(\"\\\\.\",\"\") %>% ## Remove os pontos\n    stringr::str_replace(\",\",\".\") %>%  ## Substitui a vírgula por ponto\n    as.numeric() %>% ## Converte para número\n    dbx::dbxInsert(conn,\"df\",.) ## Insere os dados na tabela já criada\n}))\n\n}\nVamos agora criar um vetor com os nomes dos arquivos a serem lidos, chamar a função read_delim_chunk dentro do walk do purrr para ler 50 mil linhas por vez, realizar as transformações e inserir na tabela previamente criada na base de dados.\na <- list.files(pattern=\"txt$\")\n\nwalk(a,~{\n\n.x %>% \nreadr::read_delim_chunked(callback = readr::DataFrameCallback$new(f),\n                         locale = readr::locale(encoding=\"latin1\"),\n                         delim = \"\\t\",\n                         chunk_size = 50000)\n\n})\nPronto, basta rodar o comando acima para deixar o R trabalhando na leitura, tranformação dos dados e inserção na base de forma segura e eficiente.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-04T19:03:19-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-r-postgresql-distinct-cheatsheet/",
    "title": "R + PostgreSQL - distinct cheatsheet",
    "description": "Este cheatsheet mostra como filtrar tanto no R\nquanto no PostgreSQL linhas distintas com base\nem uma ou mais colunas.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\n\n\nlibrary(dplyr)\nset.seed(6723)\ndf <- data.frame(col1 = sample(letters[1:5],10, replace = TRUE),\n                 col2 = sample(letters[1:5],10, replace = TRUE),\n                 col3 = sample(letters[1:5],10, replace = TRUE),stringsAsFactors = FALSE)\n\n\n\nDBI::dbWriteTable(conn,\"df\",df)\nApenas colunas selecionadas\n\n\nTidyverse\n## distinct de uma coluna, \n## retornando uma coluna\n\ndf %>% \n    distinct(col1)\n      \n##  O mesmo para mais de uma coluna  \n\ndf %>% \n  distinct(col1,col2)\n\n\n\n\n\nPostgreSQL\nSELECT DISTINCT col1\nFROM df;\nou\nSELECT col1\nFROM df,\nGROUP BY col1;\n\nSELECT DISTINCT col1,col2\nFROM df;\nou \nSELECT col1,col2\nFROM df,\nGROUP BY col2,col2;\n\n\nInclusão das demais colunas\n\n\nTidyverse\n## distinct de uma coluna, \n## retornando todas colunas  \n\ndf %>% \n  distinct(col1,.keep_all = TRUE)  \n  \n## Distinct de mais de uma coluna,\n## retornando todas colunas  \n\ndf %>% \n   distinct(col1,col2,.keep_all =TRUE )\n\n\n\n\n\nPostgreSQL\nSELECT DISTINCT ON (col1) *\nFROM df\nORDER BY col1.\n\n\nDistinct numa coluna, retornando outras colunas.\n\n\nTidyverse\ndf %>% \n  distinct(col1,.keep_all = TRUE) %>% \n  select(col1,col2)\n\n\n\n\n\nPostgreSQL\nSELECT DISTINCT ON (col1) col1,col2\nFROM df\nORDER BY col1, col2.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-04T19:14:15-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-tidypg-cheatsheet-sumarizando-tabelas-com-r-e-postgresql/",
    "title": "TidyPG cheatsheet: sumarizando tabelas com R e PostgreSQL",
    "description": "Neste cheatsheet mostro como criar sumários no R e no PostgreSQL.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\nCount\n\n\nTidyverse\n## Número de colunas\nlength(df)\n\n## Número de linhas\nnrow(df)\n\n## frequência para uma coluna\ndf %>% \n      count(col1)\n\n## frequência ordenada    \ndf %>% \n    count(col1,sort = T)\n\n##Frequência múltipla    \n\ndf %>% \n      count(col1,col2,...)\n      \n## Frequência múltipla ordenada\n\ndf %>% \n       count(col1,col2,...,sort = T)\n\n\n\n\n\nPostgreSQL\n-- Número de colunas \nSELECT COUNT(*) \nFROM information_schema.columns \nWHERE table_name = 'df'; \n\n-- Número de linhas  \nSELECT COUNT(*) \nFROM df;\n\n-- Frequência para uma coluna\n\nSELECT col1, COUNT(*) \nFROM df \nGROUP BY col1;  \n\n-- Frequência ordenada para uma coluna\nSELECT col1, COUNT(*) \nFROM df\nGROUP BY col1 \nORDER BY COUNT \nDESC;  \n\n-- Frequência múltipla\nSELECT col1, col2,... COUNT(*) \nFROM df \nGROUP BY col1, col2;\n\n-- Frequência múltipla ordenada por uma  coluna\nSELECT col1, col2,... COUNT(*) \nFROM df \nGROUP BY col1 \nORDER BY COUNT \nDESC;\n\n-- Frequência múltipla ordenada\nSELECT col1,col2, COUNT(*) \nFROM df \nGROUP BY col1, col2 \nORDER BY COUNT \nDESC;\n\n\nSummarize ou summarise\nSumário sem agrupamento\n\n\nTidyverse\nEstamos assumindo que a coluna col1 é numérica.\n\ndf %>% \n   summarize(n = n(),\n             min = min(col1),\n             max = max(col1),\n             media = mean(col1),\n             mediana = median(col1)\n             desvio_padrao = sd(col1),\n             )\n\n\n\n\n\nPostgreSQL\nBases relacionais não possuem noção intrínseca de ordem, o que torna a verificação da mediana e outras percentis ou quantis mais verboso.\nSELECT COUNT(*) AS n,  \n      MIN(col1) AS min,  \n      MAX(col1) AS max,  \n      AVG(col1) AS media,  \n      PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY col1 ASC) AS mediana,  \n      STDDEV(col1) AS desvio_padrao  \n      FROM df;\n      \n\n\nSumário com agrupamento\n\n\nTidyverse\nEstamos assumindo que a coluna col1 é numérica.\n\ndf %>% \n   group_by(grupo) %>% \n   summarize(n = n(),\n             min = min(col1),\n             max = max(col1),\n             media = mean(col1),\n             mediana = median(col1)\n             desvio_padrao = sd(col1),\n             )\n\n\n\n\n\nPostgreSQL\nBases relacionais não têm noção intrínseca de ordem, o que torna a verificação da mediana e outras percentis ou quantis mais verbosa.\nSELECT grupo, COUNT(*) AS n,  \n      MIN(col1) AS min,  \n      MAX(col1) AS max,  \n      AVG(col1) AS media,  \n      PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY col1 ASC) AS mediana,  \n      STDDEV(col1) AS desvio_padrao  \n      FROM df  \n      GROUP BY grupo;  \n      \n\n\nSumário de vetores(R)/arrays(PostgreSQL)\n\n\nR\n\nmean(c(8,7,1,4))\n\n\n\n\n\nPostgreSQL\nSELECT AVG(a) \nAS media \nFROM UNNEST(ARRAY[8,7,1,4]) \nAS a;\n      \n\n\n\n\n\n",
    "preview": "posts/2021-01-04-tidypg-cheatsheet-sumarizando-tabelas-com-r-e-postgresql/summary.png",
    "last_modified": "2021-01-04T18:49:21-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-window-functions-no-r-e-no-postgresql-parte-1-aspectos-estticos/",
    "title": "Window functions no R e no PostgreSQL - Parte 1: aspectos estáticos",
    "description": "Neste tutorial mostro como operar com funções janela ou\nwindow functions tanto no R quanto no PostgreSQL.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-04",
    "categories": [],
    "contents": "\nFunções janela aplicam uma função agregadora em uma partição de linhas selecionadas numa query. Elas retornam a mesma tabela, ou a seleção de colunas, adicionada de uma ou mais colunas com o resultado da aplicação da função agregadora, especialmente quando opera como agregado de rolagem, tal como soma cumulativa ou média móvel.\nNeste primeiro tutorial, irei mostrar como operam as principais funções. No próximo, irei explorar window frames e aplicações concretas, especialmente para séries temporais, tais como soma cumulativas e média móvel.\nSumários\nQuando você quer criar sumários dos dados no R e no PostgreSQL, a maneira mais comumumente utilizada é por meio da funções group_by e summarize no R GROUP BY e das funções de agregação no PostgreSQL.\nVamos usar o clássico dataframe mtcars para dar alguns exemplos. No R, eu faria assim para gerar as estatísticas descritivas da coluna mpg agrupadas pela coluna cyl.\n\n\nlibrary(tidyverse)\nmtcars %>% \n   group_by(cyl) %>% \n   summarize(\n   n = n(),\n   min = min(mpg),\n   max = max(mpg),\n   media = mean(mpg),\n   mediana = median(mpg),\n   desvio_padrao = sd(mpg)\n   )\n\n\n# A tibble: 3 x 7\n    cyl     n   min   max media mediana desvio_padrao\n  <dbl> <int> <dbl> <dbl> <dbl>   <dbl>         <dbl>\n1     4    11  21.4  33.9  26.7    26            4.51\n2     6     7  17.8  21.4  19.7    19.7          1.45\n3     8    14  10.4  19.2  15.1    15.2          2.56\n\nO mesmo resultado você obtêm com a query abaixo. Cuidado apenas para ordenar a coluna mpg por grupos ao calcular a mediana, pois vale sempre relembrar que SQL não possui noção intrínseca de ordem.\nSELECT cyl, COUNT(*) AS n,  \n      MIN(mpg) AS min,  \n      MAX(mpg) AS max,  \n      AVG(mpg) AS media,  \n      PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY mpg ASC) AS mediana,  \n      STDDEV(mpg) AS desvio_padrao  \n      FROM mtcars  \n      GROUP BY cyl; \nSummarize no R e as funções de agregação no PostgreSQL retornam apenas os agregados. No entanto, por vezes, queremos também as colunas originais, mesmo que os agregados se repitam. Para isso, no R utilizamos group_by com mutate:\n\n\nmtcars %>% \n   select(cyl, mpg) %>% \n   group_by(cyl) %>% \n   mutate(\n     media = mean(mpg)\n   )\n\n\n# A tibble: 32 x 3\n# Groups:   cyl [3]\n     cyl   mpg media\n   <dbl> <dbl> <dbl>\n 1     6  21    19.7\n 2     6  21    19.7\n 3     4  22.8  26.7\n 4     6  21.4  19.7\n 5     8  18.7  15.1\n 6     6  18.1  19.7\n 7     8  14.3  15.1\n 8     4  24.4  26.7\n 9     4  22.8  26.7\n10     6  19.2  19.7\n# … with 22 more rows\n\nNo PostgreSQL, este mesmo resultado pode ser obtido por meio da cláusula OVER, adicionada depois da função janela, no caso AVG, com o argumento PARTITION BY seguido da coluna agrupadora.\n SELECT mpg, cyl, AVG(mpg)\n OVER (PARTITION BY cyl) FROM mtcars;\nVocê deve ter notado que eu criei apenas um sumário. Para criar vários sumários, com o mutate é simples:\n\n\nmtcars %>% \n   select(cyl,mpg) %>% \n   group_by(cyl) %>% \n   mutate(\n   n = n(),\n   min = min(mpg),\n   max = max(mpg),\n   media = mean(mpg),\n   mediana = median(mpg),\n   desvio_padrao = sd(mpg)\n   )\n\n\n# A tibble: 32 x 8\n# Groups:   cyl [3]\n     cyl   mpg     n   min   max media mediana desvio_padrao\n   <dbl> <dbl> <int> <dbl> <dbl> <dbl>   <dbl>         <dbl>\n 1     6  21       7  17.8  21.4  19.7    19.7          1.45\n 2     6  21       7  17.8  21.4  19.7    19.7          1.45\n 3     4  22.8    11  21.4  33.9  26.7    26            4.51\n 4     6  21.4     7  17.8  21.4  19.7    19.7          1.45\n 5     8  18.7    14  10.4  19.2  15.1    15.2          2.56\n 6     6  18.1     7  17.8  21.4  19.7    19.7          1.45\n 7     8  14.3    14  10.4  19.2  15.1    15.2          2.56\n 8     4  24.4    11  21.4  33.9  26.7    26            4.51\n 9     4  22.8    11  21.4  33.9  26.7    26            4.51\n10     6  19.2     7  17.8  21.4  19.7    19.7          1.45\n# … with 22 more rows\n\nCom funções janela, é um pouquinho mais complicado.\nselect mpg,cyl, \nCOUNT(*)  over w as n,\nMIN(mpg)  over w as min,\nMAX(mpg)  over w as max,\nAVG(mpg) over w as media,\nSTDDEV(mpg) over w as desvio_padrao\nfrom mtcars\nwindow w as  (partition by cyl);\nOu seja, para criar vários sumários, é necessário criar uma cláusula WINDOW e referenciá-la com OVER em cada um dos agregados.\nWindow functions\nAlém das funções de agregação, tais como mean, median, sd, outras funções operam como funções janela. Abaixo segue a lista delas:\n\n\n\n\nnome_sql\nnome_dplyr\ndescricao\nCUME_DIST\ncume_dist\nRetorna a classificação relativa da linha atual\nDENSE_RANK\ndense_rank\nClassifica a linha atual dentro da partição sem intervalos\nFIRST_VALUE\nfirst\nRetorna o valor avaliado em relação à primeira linha dentro da partição\nLAG\nlag\nRetorna o valor avaliado na linha que está em uma distância física especificada antes da linha atual dentro da partição\nLAST_VALUE\nlast\nRetorna o valor avaliado em ralação à última linha dentro da partição\nLEAD\nlead\nRetorna o valor avaliado na linha que está a certa distância depois da linha atual dentro da partição\nNTILE\nntile\nDivide linhas em uma partição, tanto quanto possível, em iguais quantidades, e atribui a cada linha um inteiro começando por um até o valor do argumento\nNTH_VALUE\nnth\nRetorna o valor avaliado em relação à linha nth em partição ordenada\nPERCENT_RANK\npercent_rank\nRetona a classificação relativa da linha atual (rank-1) / (total de linhas -1)\nRANK\nmin_rank\nRetorna a linha atual dentro da partição com intervalos\nROW_NUMBER\nrow_number\nNúmero da linha atual dentro da partição começando por 1\n\nEssas funções regulamente supõem a existência de linhas ordenadas, que no SQL não existe. Se para você, classificar as linhas conforme uma ordem é importante, será necessário adicionar a cláusula ORDER BY dentro do OVER, a fim de assegurar a classificação segundo uma ordem. No R, a ordem é fixa, de modo que, se você já está satisfeita com ordem das linhas, não é necessário argumento adicional.\nVamos dar uma olhada nessas funções conforme o uso.\nRow Number\nEsta função retorna uma sequência numérica, começando por um, dos valores agrupados.\n\n\n  mtcars %>% \n  select(cyl,mpg) %>% \n  group_by(cyl) %>% \n  mutate(classificacao = row_number(mpg)) %>% \n  arrange(cyl,classificacao)\n\n\n# A tibble: 32 x 3\n# Groups:   cyl [3]\n     cyl   mpg classificacao\n   <dbl> <dbl>         <int>\n 1     4  21.4             1\n 2     4  21.5             2\n 3     4  22.8             3\n 4     4  22.8             4\n 5     4  24.4             5\n 6     4  26               6\n 7     4  27.3             7\n 8     4  30.4             8\n 9     4  30.4             9\n10     4  32.4            10\n# … with 22 more rows\n\nEu coloquei o mpg dentro do row_number. Você pode retirar, mas aí a classificação respeitará a ordem original. Outra opção, é colocar o arrange antes do mutate. Dessa forma, você não precisa incluir a coluna dentro do row_number.\nVejamos como realizar a mesma operação no PostgreSQL.\nSELECT cyl, mpg, \nROW_NUMBER() OVER (PARTITION BY cyl ORDER BY mpg) FROM mtcars;\nmin_rank e rank\nCom rank, cria-se uma sequência também começando por um, com a diferença de que, ele retorna a mesma classificação para valores idênticos no mesmo grupo. Além disso, ele realiza saltos.\nEu não sei porque razão, mas rank no PostgreSQL não corresponde a exatamente rank no dplyr, mas a min_rank, ou rank com o argumento ties.method = “min”. Se você usar rank sem argumento, dplyr irá dividir um pelo número de valores identicos repetidos e adicionar ao inteiro da classificação. Veja no exemplo abaixo.\n\n\nmtcars %>% \n  select(mpg,cyl) %>% \n  group_by(cyl) %>% \n    arrange(cyl,mpg) %>% \n  mutate(classificacao = rank(mpg))\n\n\n# A tibble: 32 x 3\n# Groups:   cyl [3]\n     mpg   cyl classificacao\n   <dbl> <dbl>         <dbl>\n 1  21.4     4           1  \n 2  21.5     4           2  \n 3  22.8     4           3.5\n 4  22.8     4           3.5\n 5  24.4     4           5  \n 6  26       4           6  \n 7  27.3     4           7  \n 8  30.4     4           8.5\n 9  30.4     4           8.5\n10  32.4     4          10  \n# … with 22 more rows\n\nNote que 22.8 com cyl 4 se repetem. Como eles aparecem pela primeira vez na terceira posição e só há uma repetição, o número 3 é adicionado de 0,5. Provavelmente, o que, na verdade, você quer é usar min_rank:\n\n\nmtcars %>% \n  select(mpg,cyl) %>% \n  group_by(cyl) %>% \n  arrange(cyl,mpg) %>% \n  mutate(classificacao = min_rank(mpg))\n\n\n# A tibble: 32 x 3\n# Groups:   cyl [3]\n     mpg   cyl classificacao\n   <dbl> <dbl>         <int>\n 1  21.4     4             1\n 2  21.5     4             2\n 3  22.8     4             3\n 4  22.8     4             3\n 5  24.4     4             5\n 6  26       4             6\n 7  27.3     4             7\n 8  30.4     4             8\n 9  30.4     4             8\n10  32.4     4            10\n# … with 22 more rows\n\nResultado idêntico você obtêm com o PostgreSQL utilizando apenas a função RANK:\nSELECT mpg, cyl,\nRANK() OVER (PARTITION BY cyl ORDER BY mpg) FROM mtcars;\nNote que o número quatro é saltado na coluna classificação. Isso porque o segundo 3 ocupa o seu lugar.\ndense_rank\nDense_rank é similar a rank, com a diferença de que não há saltos.\n\n\nmtcars %>% \n   select(mpg,cyl) %>% \n   group_by(cyl) %>% \n   arrange(cyl,mpg) %>% \n   mutate(classificacao = dense_rank(mpg))\n\n\n# A tibble: 32 x 3\n# Groups:   cyl [3]\n     mpg   cyl classificacao\n   <dbl> <dbl>         <int>\n 1  21.4     4             1\n 2  21.5     4             2\n 3  22.8     4             3\n 4  22.8     4             3\n 5  24.4     4             4\n 6  26       4             5\n 7  27.3     4             6\n 8  30.4     4             7\n 9  30.4     4             7\n10  32.4     4             8\n# … with 22 more rows\n\nAgora no PostgreSQL\nSELECT mpg, cyl,\nDENSE_RANK() OVER (PARTITION BY cyl ORDER BY mpg) FROM mtcars;\nPercent_rank\nPercent_rank retorna a classificação relativa da linha atual, num intervalo de zero a um. O cálculo é feito da seguinte forma: (rank-1)/total de linhas na partição -1).\n\n\nmtcars %>% \n   select(mpg,cyl) %>% \n   group_by(cyl) %>% \n   arrange(cyl,mpg) %>% \n   mutate(classificacao = percent_rank(mpg))\n\n\n# A tibble: 32 x 3\n# Groups:   cyl [3]\n     mpg   cyl classificacao\n   <dbl> <dbl>         <dbl>\n 1  21.4     4           0  \n 2  21.5     4           0.1\n 3  22.8     4           0.2\n 4  22.8     4           0.2\n 5  24.4     4           0.4\n 6  26       4           0.5\n 7  27.3     4           0.6\n 8  30.4     4           0.7\n 9  30.4     4           0.7\n10  32.4     4           0.9\n# … with 22 more rows\n\nNo PostgreSQL:\nSELECT cyl,mpg,\nPERCENT_RANK() OVER (PARTITION BY cyl ORDER BY mpg) FROM mtcars;\nFirst, last e nth\nEstas funções retornam o primeiro, o último e valor n dentro da partição no R. First_value irá retornar o primeiro valor no postgreSQL, mas last_value e nth_value poderão lhe surpreender.\n\n\nmtcars %>% \n   select(cyl,mpg) %>% \n   group_by(cyl) %>% \n   arrange(cyl,mpg) %>% \n   mutate(primeiro_valor = first(mpg),\n      valor_5 = nth(mpg,5),\n      ultimo_valor = last(mpg))\n\n\n# A tibble: 32 x 5\n# Groups:   cyl [3]\n     cyl   mpg primeiro_valor valor_5 ultimo_valor\n   <dbl> <dbl>          <dbl>   <dbl>        <dbl>\n 1     4  21.4           21.4    24.4         33.9\n 2     4  21.5           21.4    24.4         33.9\n 3     4  22.8           21.4    24.4         33.9\n 4     4  22.8           21.4    24.4         33.9\n 5     4  24.4           21.4    24.4         33.9\n 6     4  26             21.4    24.4         33.9\n 7     4  27.3           21.4    24.4         33.9\n 8     4  30.4           21.4    24.4         33.9\n 9     4  30.4           21.4    24.4         33.9\n10     4  32.4           21.4    24.4         33.9\n# … with 22 more rows\n\nDa primeira vez que eu usei o PostgreSQL para obter o último e o valor n dentro da partição, fiquei bastante supreso. Veja o resultado abaixo\nSELECT cyl,mpg,\nFIRST_VALUE(mpg) OVER w,\nLAST_VALUE(mpg) OVER w,\nNTH_VALUE(mpg,5) OVER w\nFROM mtcars\nWINDOW w AS (PARTITION BY cyl ORDER BY mpg);\n\ncyl\nmpg\nprimeiro_valor\nvalor_5\nultimo_valor\n4\n21.4\n21.4\nNA\n21.4\n4\n21.5\n21.4\nNA\n21.5\n4\n22.8\n21.4\nNA\n22.8\n4\n22.8\n21.4\nNA\n22.8\n4\n24.4\n21.4\n24.4\n24.4\n4\n26\n21.4\n24.4\n26\n4\n27.3\n21.4\n24.4\n27.3\n4\n30.4\n21.4\n24.4\n30.4\n4\n30.4\n21.4\n24.4\n30.4\n4\n32.4\n21.4\n24.4\n32.4\n4\n33.9\n21.4\n24.4\n33.9\n6\n17.8\n17.8\nNA\n17.8\n6\n18.1\n17.8\nNA\n18.1\n6\n19.2\n17.8\nNA\n19.2\n6\n19.7\n17.8\nNA\n19.7\n6\n21\n17.8\n21\n21\n6\n21\n17.8\n21\n21\n6\n21.4\n17.8\n21\n21.4\n8\n10.4\n10.4\nNA\n10.4\n8\n10.4\n10.4\nNA\n10.4\n8\n13.3\n10.4\nNA\n13.3\n8\n14.3\n10.4\nNA\n14.3\n8\n14.7\n10.4\n14.7\n14.7\n8\n15\n10.4\n14.7\n15\n8\n15.2\n10.4\n14.7\n15.2\n8\n15.2\n10.4\n14.7\n15.2\n8\n15.5\n10.4\n14.7\n15.5\n8\n15.8\n10.4\n14.7\n15.8\n8\n16.4\n10.4\n14.7\n16.4\n8\n17.3\n10.4\n14.7\n17.3\n8\n18.7\n10.4\n14.7\n18.7\n8\n19.2\n10.4\n14.7\n19.2\n\nNote que, o primeiro valor retornou os valores esperado. Já last_value retornou os mesmos valores de mpg, e nth_value retornou NULL até o quarto valor (Aparece NA pq importei o resultado para o R). Isso aconteceu por causa de algo chamado windown frame, que contêm uma cláusula implícita, cada vez que chamamos a cláusula OVER. Veremos no próximo tutorial o que são window frames. Por ora, vamos aprender a contornar esse resultado.\nPara obter resultado idêntido ao dplyr, devemos alterar a moldura da janela (window frame):\nSELECT cyl,mpg,\nFIRST_VALUE(mpg) OVER w,\nNTH_VALUE(mpg,5) OVER w,\nLAST_VALUE(mpg) OVER w\nFROM mtcars\nWINDOW w AS (\nPARTITION BY cyl ORDER BY mpg\nRANGE BETWEEN UNBOUNDED  PRECEDING AND UNBOUNDED FOLLOWING\n);\nLag e lead\nDuas funções extremamente úteis para quem trabalha com séries temporais são lag e lead. A primeira retorna o valor imediatamente anterior ao atual, por padrão, ou o valor distante n linhas. Lead funciona de forma similar, mas para valores posteriores.\n\n\nmtcars %>% \n   select(mpg,cyl) %>% \n   group_by(cyl) %>% \n   arrange(cyl,mpg) %>% \n   mutate(anterior = lag(mpg),\n          posterior = lead(mpg))\n\n\n# A tibble: 32 x 4\n# Groups:   cyl [3]\n     mpg   cyl anterior posterior\n   <dbl> <dbl>    <dbl>     <dbl>\n 1  21.4     4     NA        21.5\n 2  21.5     4     21.4      22.8\n 3  22.8     4     21.5      22.8\n 4  22.8     4     22.8      24.4\n 5  24.4     4     22.8      26  \n 6  26       4     24.4      27.3\n 7  27.3     4     26        30.4\n 8  30.4     4     27.3      30.4\n 9  30.4     4     30.4      32.4\n10  32.4     4     30.4      33.9\n# … with 22 more rows\n\nNo PostgreSQL:\nSELECT mpg,cyl,\nlag(mpg) OVER w,\nlead(mpg) OVER w\nFROM mtcars\nWINDOW w AS (PARTITION BY cyl ORDER BY mpg);\nNtile\nPor fim, resta falar de ntile, que eu particularmente considero muito útil para identificar grupos num dataframe a fim de paralelizar operações no R. Tanto no PostgreSQL quanto no R, faz também sentido usar sem PARTITION BY ou group_by respectivamente, pois assim a tabela inteira é agrupada. Note que ntile cria grupos, tanto quanto possível, iguais.\n\n\nmtcars %>% \n   select(mpg,cyl) %>% \n   arrange(cyl,mpg) %>% \n   mutate(grupo = ntile(n=4))\n\n\n    mpg cyl grupo\n1  21.4   4     1\n2  21.5   4     1\n3  22.8   4     1\n4  22.8   4     1\n5  24.4   4     1\n6  26.0   4     1\n7  27.3   4     1\n8  30.4   4     1\n9  30.4   4     2\n10 32.4   4     2\n11 33.9   4     2\n12 17.8   6     2\n13 18.1   6     2\n14 19.2   6     2\n15 19.7   6     2\n16 21.0   6     2\n17 21.0   6     3\n18 21.4   6     3\n19 10.4   8     3\n20 10.4   8     3\n21 13.3   8     3\n22 14.3   8     3\n23 14.7   8     3\n24 15.0   8     3\n25 15.2   8     4\n26 15.2   8     4\n27 15.5   8     4\n28 15.8   8     4\n29 16.4   8     4\n30 17.3   8     4\n31 18.7   8     4\n32 19.2   8     4\n\nNo PostgreSQL, você obtêm o mesmo resultado com:\nSELECT mpg,cyl,\nNTILE(3) OVER (ORDER BY mpg) FROM mtcars;\nConclusão\nExploraramos as principais funcionalidades e nos detivemos mais nos aspectos estáticos. Funções janela possuem aplicações infindáveis. Iremos mostrar no próximo tutorial como funcionam as molduras de janela ou window frames, as quais permitem realizar excelentes operações, especialmente com séries temporais, que regularmente requerem transformações dinâmicas, tais soma cumulativa e média móvel.\nAtenção\nFunções janela são os últimos ítens a serem avalidados numa query, com exceção da cláusula ORDER BY. Por serem a última avaliação, funções janela não podem ser usadas dentro de cláusulas WHERE, GROUP BY ou HAVING.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-04T18:54:55-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-joins-no-r-e-no-postgresql/",
    "title": "Joins no R e no PostgreSQL",
    "description": "Neste tutorial, mostro como as principais formas de joins tanto no R\nquanto no PostgreSQL.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-03",
    "categories": [],
    "contents": "\nIntrodução\nUm dos propósitos do curso sobre ciência de dados com R e PostgreSQL é mostrar como realizar as mesmas tarefas nas duas linguagens. Um bom exemplo são as junções ou joins, as quais permitem combinar tabelas ou vetores para gerar uma nova tabela. Você pode emparelhá-las verticalmente, horizontalmente ou criar um produto cartesiano.\nNo emparelhamento vertical, exige-se que as colunas tenham o mesmo nome e os tipos sejam os mesmos. No horizontal, exige-se que os tamanhos da colunas ou vetores sejam os mesmos. Desses tipos de combinações, tratamos em outro tutorial.\nNeste tutorial, iremos falar de combinações, ou melhor, junções, que pressupõem a comparação entre valores de colunas conforme especificação pelo usuário. Essas junções criam tabelas novas, as quais não se confundem com as tabelas originais e que não necessariamente preservam todas as colunas e todas as linhas. Por esta razão, é importante ter claro o que se quer ao juntar tabelas para evitar resultados indesejados. A imagem abaixo ilustra uma operação de inner join.\n\n\n\nJunções no R e no PostgreSQL podem ser usadas para diferentes propósitos. No R junções são úteis no processo de transformação dos dados, ou seja, geralmente extraímos dados de diferentes fontes e queremos juntá-los para compor novos dados. Por vezes, a junção é um passo intermediário na programação sem qualquer fim em si mesma.\nJá no PostgreSQL, a junção é geralmente realizada nas buscas de tabelas normalizadas, ou seja, quando as informações a serem passadas para o cliente via query estão dispersas em várias tabelas. Ademais, servem para denormalizar o banco a fim de melhorar o desempenho de queries.\nEm suma, no R junções são especialmente usadas como técnica de programação e manuseio de dados, em SQL como estratégia de armazenamento e extração (queries) de dados.\nAlém disso, como no R a ordem em que aparecem os valores num vetor importa, adicionar uma coluna ao dataframe é uma mera questão de emparelhamento, ou seja, usando df$nova_coluna <- vetor_externo ou tibble::add_column(df,nova_coluna = vetor_externo). Por sua vez, SQL não possui noção de ordem, de modo que o emparelhamento horizontal se faz por meio de joins.\nAntes de realizar joins, tente responder para si mesma questões como:\n1 - Quero preservar todas as linhas das duas tabelas?\n2 - Quero preservar todas as linhas de apenas uma tabela?\n3 - Quero preservar todas as colunas de ambas tabelas?\n4 - Quero preservar NAs (no R) ou NULLs no PostgreSQL?\n5 - Quero juntar as tabelas com base na comparação de apenas um par de colunas, ou seja, coluna a da tbl1 com coluna b da tbl2?\n6 - Se fizer conforme a pergunta 5, o que vai acontecer com colunas com o mesmo nome nas duas tabelas, mas que não pretendo comparar para efeitos de junção?\n7 - Quero valores das duas tabelas ou apenas de uma?\nPacotes necessários\nPara você não ser tomada de surpresa, vamos carregar os pacotes necessários logo no início. Assim, se você não tiver algum deles, trate de instalá-los.\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(pander)\nlibrary(DBI)\n\n\n\nJoins no R com Tidyverse\nNo R base, existe a função merge. No entanto, eu irei adotar a abordagem tidyverse para os joins. Uma das razões para usar o tidyverse é o fato de ele ser bem documentado.\nUm dos pacotes mais importantes do tidyverse é o dplyr. Ele possui dois grupos de joins. Um grupo de joins de mutação, composto pelos seguintes joins:\n1 - inner_join = mantêm todas as linhas de x e de y;\n2 - left_join = mantêm todas as linhas de x;\n3 - right_join = mantêm todas as linhas de y;\n4 - full_join = mantêm todas as linhas de x e de y.\nO segundo grupo é formado por joins de filtragem. A caractística desses joins é que eles retornam apenas o primeiro dataframe, filtrado com base na comparação com outro dataframe.\n1 - semi_join = retorna todas as linhas x que encontram correpondentes em y;\n2 - anti_join = retorna todas as linhas de x que não encontram correspondentes em y.\nHá uma terceira forma de join chamada self_join, que na verdade é um inner_join de uma tabela com ela mesma.\nVamos iniciar por criar dois dataframes. Mais adiante enviaremos para o PostgreSQL.\n\n\nset.seed(035)\nd1 <- tibble::tibble(a=sample(1:5,5),\n                     b=sample(c(NA_real_,6:9),5),\n                     c=sample(c(NA_character_,sample(letters,4)),5))\npander::pander(d1)\n\n\na\nb\nc\n2\nNA\nj\n5\n6\ni\n1\n9\nNA\n3\n8\nu\n4\n7\nq\n\nPerceba que eu incluí NAs propositalmente para aprendermos como tratar deles tanto no R quanto no PostgreSQL.\n\n\nset.seed(936)\nd2 <- tibble::tibble(a= sample(c(NA_real_,2:7),7),b=sample(c(NA_real_,3:8),7))\npander::pander(d2)\n\n\na\nb\n4\n5\n2\n3\n7\n6\n6\n8\n3\nNA\n5\n7\nNA\n4\n\nOs dois dataframes possuem duas colunas com o mesmo nome a e b, porém o dataframe d1 possui uma coluna a mais, chamada c.\nInner join com Tidyverse\nInner join comparando apenas um par de colunas.\nNo inner_join, o dataframe resultante mantêm todas as linhas dos dois outros dataframes onde os valores das colunas a serem comparadas, que doravante chamaremos chaves, são comuns, descartando as linhas de um que não encontram correspondente no outro.\n\n\nd3 <- inner_join(d1,d2,by = \"a\")\npander(d3)\n\n\na\nb.x\nc\nb.y\n2\nNA\nj\n3\n5\n6\ni\n7\n3\n8\nu\nNA\n4\n7\nq\n5\n\nComo a coluna b está presente nos dois dataframes, mas não foi usada para comparação, o R tratou de renomeá-las para indicar que b.x vem do primeiro dataframe e b.y, do segundo.\nFrequentemente, eu me vejo na situação em que a coluna de um dataframe contêm valores que quero usar para substitir valores ausentes (NAs) em outra coluna do dataframe.\nAo dar o join acima, eu não fiz referência à coluna b presente nos dois dataframes, pois quero preservar o valores originais para, posteriormente, substituir os NAs.\nPara tanto, vem à calhar a função coalesce. Ela serve para substitir os valores ausentes de uma coluna ou de um vetor por valores de outra coluna ou vetor e vice-versa. Vamos substituir os valores ausentes das colunas b.x por seus correspondentes na coluna b.y.\n\n\nd3 <- d3 %>% \n  mutate(b = coalesce(b.x,b.y))\npander(d3)\n\n\na\nb.x\nc\nb.y\nb\n2\nNA\nj\n3\n3\n5\n6\ni\n7\n6\n3\n8\nu\nNA\n8\n4\n7\nq\n5\n7\n\nQuando ambas colunas possuem valores existentes, coalesce preserva os da primeira e descarta os da segunda. Então, se você quiser preservar os da segunda, é só inverter a ordem dos argumentos.\nInner join comparando mais de um par de colunas.\nO procedimento aqui é similiar, porém mais estrito, pois se exige que dois valores numa mesma linha encontrem correspondentes no outro dataframe. No caso, não houve correspondência e o retorno foi um dataframe com zero linhas.\n\n\ninner_join(d1,d2,by=c(\"a\",\"b\")) %>% \n  pander()\n\n\na\nb\nc\n\nNote que, no dataframe d1, na linha em que a coluna a contêm o valor 3, a coluna b contêm o valor 8. Por sua vez, no dataframe d2, na linha em que a coluna a contêm o valor 3, a coluna b é NA. O inner_join ao comparar um valor conhecido com um NA retorna NA e acaba por descartar aquela linha.\nNeste caso, você pode omitir o argumento by e o R irá interpretar que você quer realizar o join com base em todas as colunas do mesmo nome de ambos dataframes. Esse join é chamado de natural join em SQL. No R será emitida uma mensagem indicando quais as colunas foram utilizadas para o join:\n\n\ninner_join(d1,d2) %>% \n  pander()\n\n\nJoining, by = c(\"a\", \"b\")\na\nb\nc\n\nPara evitar surpresas, é recomendável adotar a primeira abordagem, qual seja, explicitar as chaves.\nInner joins com nomes diferentes de chaves.\nAgora suponha que você queira dar um join por duas colunas com nomes diferentes, a família join do dplyr possui uma uma sintaxe bem particular. Eu vou criar um novo dataframe a partir do d1 e renomear a coluna a para aa, para então compará-la com a coluna a do dataframe b.\n\n\nd4 <- d1 %>% \n  select(aa = a, everything())\npander(d4)\n\n\naa\nb\nc\n2\nNA\nj\n5\n6\ni\n1\n9\nNA\n3\n8\nu\n4\n7\nq\n\n\n\nd5 <- inner_join(d4,d2,by=c(\"aa\"=\"a\"))\npander(d5)\n\n\naa\nb.x\nc\nb.y\n2\nNA\nj\n3\n5\n6\ni\n7\n3\n8\nu\nNA\n4\n7\nq\n5\n\nComo você pode observar, o nome preservado é o do dataframe da esquerda.\nLeft join e right join\nAs funções left_join e right_join são similares. A primeira preserva todas a linhas do dataframe à da esquerda, a segunda, da direita.\n\n\nd6 <- left_join(d1,d2, by = \"a\")\npander(d6)\n\n\na\nb.x\nc\nb.y\n2\nNA\nj\n3\n5\n6\ni\n7\n1\n9\nNA\nNA\n3\n8\nu\nNA\n4\n7\nq\n5\n\n\n\nd7 <- right_join(d1,d2, by=\"a\")\npander(d7)\n\n\na\nb.x\nc\nb.y\n2\nNA\nj\n3\n5\n6\ni\n7\n3\n8\nu\nNA\n4\n7\nq\n5\n7\nNA\nNA\n6\n6\nNA\nNA\n8\nNA\nNA\nNA\n4\n\nÉ importante notar que, tanto no left join e right join quanto no full join, o R realiza um coalesce entre as chaves. Se você quiser preservar as colunas originais, adicione o argumento keep = TRUE:\n\n\nd8 <- left_join(d1,d2,by = \"a\",keep = TRUE)\npander(d8)\n\n\na.x\nb.x\nc\na.y\nb.y\n2\nNA\nj\n2\n3\n5\n6\ni\n5\n7\n1\n9\nNA\nNA\nNA\n3\n8\nu\n3\nNA\n4\n7\nq\n4\n5\n\nFull join\nDiferentemente dos anteriores, full join preserva todas as linhas de ambos dataframes, encontrando ou não valores correspondentes.\n\n\nd9 <- full_join(d1,d2, by = \"a\")\npander(d9)\n\n\na\nb.x\nc\nb.y\n2\nNA\nj\n3\n5\n6\ni\n7\n1\n9\nNA\nNA\n3\n8\nu\nNA\n4\n7\nq\n5\n7\nNA\nNA\n6\n6\nNA\nNA\n8\nNA\nNA\nNA\n4\n\n\n\nd10 <- full_join(d1,d2, by=c(\"a\",\"b\"))\npander(d10)\n\n\na\nb\nc\n2\nNA\nj\n5\n6\ni\n1\n9\nNA\n3\n8\nu\n4\n7\nq\n4\n5\nNA\n2\n3\nNA\n7\n6\nNA\n6\n8\nNA\n3\nNA\nNA\n5\n7\nNA\nNA\n4\nNA\n\nJoins de filtragem\nJoins de filtragem servem para filtrar as linhas de um dataframe com base em seus correspondentes no outro dataframe. Eles não retornam nada do segundo dataframe, apenas do primeiro.\nFunção semi_join\n\n\nd11 <- semi_join(d1,d2,by = \"a\")\npander(d11)\n\n\na\nb\nc\n2\nNA\nj\n5\n6\ni\n3\n8\nu\n4\n7\nq\n\nVeja que as colunas de d2 foram dispensadas. Semi join é similar à seguinte operação:\n\n\nd12 <- d1 %>% \n      filter(a %in% d2$a)\npander(d12)\n\n\na\nb\nc\n2\nNA\nj\n5\n6\ni\n3\n8\nu\n4\n7\nq\n\nAnti join\nAnti join, como o próprio nome indica, retorna apenas as linhas de um dataframe cuja chave não encontra correspondente no segundo dataframe.\n\n\nd13 <- anti_join(d1,d2, by = \"a\")\n\npander(d13)\n\n\na\nb\nc\n1\n9\nNA\n\nAnti join é equivalente a:\n\n\nd14 <- d1 %>% \n      filter(!a %in% d2$a)\npander(d14)\n\n\na\nb\nc\n1\n9\nNA\n\nJoins no PostgreSQL\nINNER JOIN\nInner join no PostgreSQL é muito similar ao inner join no R, com a diferença de que, se houver colunas com mesmo nome que não forem usadas para comparação, elas são mantidas com o mesmo nome, o que pode gerar confusão.\nQuanto à sintaxe, ela é bem intuitiva. Se quiser juntar todas as colunas tanto de uma tabela, quanto da outra, basta colocar um asterisco depois do SELECT, indicar a primeira tabela com FROM, chamar o INNER JOIN e indicar a segunda tabela. Para indicar uma ou mais chaves, coloque-as entre parênteses depois de USING.\nINNER JOIN com uma chave\nSELECT * FROM d1 INNER JOIN d2 USING (a);\na\nb\nc\nb\n2\n\nj\n3\n3\n8\nu\n\n4\n7\nq\n5\n5\n6\ni\n7\nINNER JOIN com mais de uma chave\nSELECT * FROM d1 INNER JOIN d2 USING (a,b);\na\nb\nc\n(0 rows)\nPara obter o resultado com colunas diferentes, você pode juntar a primeira tabela com um subquery da segunda, pelo qual você alterou os nomes apropriadamente.\nSELECT * FROM d1 INNER JOIN (SELECT a, b AS b_y FROM d2) AS foo  USING (a);\na\nb\nc\nb_y\n2\n\nj\n3\n3\n8\nu\n\n4\n7\nq\n5\n5\n6\ni\n7\n(4 rows)\nNATURAL JOIN\nAssim como no R, se você omitir o by, o join acontecerá com todas as colunas com o mesmo nome. No PostgreSQL, você obtêm o mesmo resultado informando que se trata de um natural join:\nSELECT * FROM d1 NATURAL INNER JOIN d2;\na\nb\nc\n(0 rows)\nOUTER JOINS\nSob o nome genérico de OUTER JOIN, encontram-se LEFT E RIGHT e FULL joins. A palavra OUTER pode ser omitida no query.\nBasicamente o que ele faz é realizar um INNER JOIN no primeiro momento e, em seguida, um novo JOIN é realizado para conter os valores da tabela de referência e NULL da tabela seguinte.\n\nSELECT * FROM  d1 LEFT OUTER JOIN d2 USING (a);\na\nb\nc\nb\n1\n9\n\n\n2\n\nj\n3\n3\n8\nu\n\n4\n7\nq\n5\n5\n6\ni\n7\n(5 rows)\nA cláusula USING não é mais que um atalho para d1.a = d2.a:\nSELECT * FROM  d1 LEFT OUTER JOIN d2 on d1.a = d2.a;\nOu\nSELECT * FROM  d1 LEFT OUTER JOIN d2 ON d1.a = d2.a AND d1.b = d2.b;\nIgualmente, se você quiser realizar um NATURAL JOIN:\n\nselect * from d1 natural left outer join d2;\na\nb\nc\n1\n9\n\n2\n\nj\n3\n8\nu\n4\n7\nq\n5\n6\ni\n(5 rows)\nVocê pode também usar a cláusula WHERE para filtrar depois do join:\nSELECT * FROM d1 LEFT JOIN d2 ON t1.a = t2.a WHERE t2.b = 3;\na\nb\nc\na\nb\n2\n\nj\n2\n3\n(1 row)\nFULL JOIN segue o mesmo padrão anterior:\n\nSELECT * FROM  d1 FULL OUTER JOIN d2 USING (a);\na\nb\nc\nb\n1\n9\n\n\n2\n\nj\n3\n3\n8\nu\n\n4\n7\nq\n5\n5\n6\ni\n7\n6\n\n\n8\n7\n\n\n6\n\n\n4\n\n(8 rows)\nMuito mais pode ser feito com joins tanto no R quanto no PostgreSQL, mas este já é um bom começo.\nAnti Join no PostgreSQL\nO PostgreSQL permite quatro formas de anti joins. Vamos mostrar cada uma delas. Eu usei este post como referência.\nNOT EXISTS\nRápida, segura e eficiente, prefira esta sempre que possível.\nSELECT * FROM d1 \nWHERE NOT EXISTS(\nSELECT  -- Não precisa selecionar coluna alguma.\nFROM d2\nWHERE a = d1.a);\na\nb\nc\n1\n9\n\n(1 row)\nLEFT JOIN com IS NULL\nEsta forma é tão rápida quanto, mas não tão eficiente, pois pode retornar colunas extras.\nSELECT * FROM d1\nLEFT JOIN d2 using(a)\nWHERE d2.a is NULL;\na\nb\nc\nb\n1\n9\n\n\n(1 row)\nEXCEPT ALL\nSe para você basta retornar a coluna de comparação, use execpt all:\nSELECT a\nFROM d1\nEXCEPT ALL\nSELECT a\nFROM d2;\na\n1 (1 row)\nNOT IN\nParticularmente, acho mais intuitiva, mas é mais arriscada, pois pode retornar resultados indesejados se houver NULL em um dos lados. Vejamos um exemplo em o resultado é inexperado.\nSELECT * \nFROM d1\nWHERE a NOT IN (\nSELECT a \nFROM d2);\na\nb\nc\n(0 rows)\nIsto aconteceu porque há nulos na coluna a da tabela d2. Para não correr este risco, assegure-se de ajustar para não nulos na referida coluna:\n\nSELECT *\nFROM d1\nWHERE a NOT IN (\nSELECT a\nFROM d2\nWHERE a IS NOT NULL);\na\nb\nc\n1\n9\n\n(1 row)\nSELF JOIN\nSelf joins são nada mais que joins de uma tabela com ela mesma. Este join é particularmente útil quando temos valores comuns entre duas colunas distintas. Uma situação típica é a de quando temos numa coluna o id de funcionários e na outra se seus respectivos supervisores, sendo esses também funcionários. Vejamos um exemplo.\n\n\nfunc <- tibble::tibble(\n  \n  id_funcionario = c(1:8),\n  nome = c(\"Heloísa\",\"Daniel\",\"Thandara\",\"Naiara\",\"Patrick\",\"Haydee\",\"Julia\",\"Fabio\"),\n  id_supervisor = c(NA_real_,1,1,2,2,3,3,3)\n)\npander(func)\n\n\nid_funcionario\nnome\nid_supervisor\n1\nHeloísa\nNA\n2\nDaniel\n1\n3\nThandara\n1\n4\nNaiara\n2\n5\nPatrick\n2\n6\nHaydee\n3\n7\nJulia\n3\n8\nFabio\n3\n\nSelf join no R\nSelf join no R não é mais que um inner join combinando uma coluna, id do suprvisor, da primeira tabela com outra coluna, id do funcionário, da segunda tabela. Aproveitamos para selecionar as colunas de interesse e nomeá-las adequadamento.\n\n\ndplyr::inner_join(func,func,by = c(\"id_supervisor\"= \"id_funcionario\")) %>%\n  dplyr::select(funcionario = nome.x, supervisor = nome.y) %>% \n  pander()\n\n\nfuncionario\nsupervisor\nDaniel\nHeloísa\nThandara\nHeloísa\nNaiara\nDaniel\nPatrick\nDaniel\nHaydee\nThandara\nJulia\nThandara\nFabio\nThandara\n\nSelf join no PostgreSQL\nSelf join no PostgreSQL se dá da mesma forma. A diferença é que a sintaxe muda um pouquinho, mas o resultado é exatamente o mesmo.\n\nSELECT f.nome funcionario, s.nome supervisor\n       FROM func f \n       JOIN func s ON s.id_funcionario = f.id_supervisor; \n\n\nfuncionario\nsupervisor\nDaniel\nHeloísa\nThandara\nHeloísa\nNaiara\nDaniel\nPatrick\nDaniel\nHaydee\nThandara\nJulia\nThandara\nFabio\nThandara\n\nConsiderações finais\nJoins no R são mais fáceis, mas têm suas limitações de desempenho. Por sua vez, no PostgreSQL o ganho de desempenho é extraordinário quando o banco está normalizado, com criação de índices e restrições, especificamente chaves primárias e estrangeiras. Esses dois recursos, quando usados apropriadamente, reduzem significativamente o tempo de resposta. Sem contar outros benefícios, como alta disponibilidade, nível de organização e segurança dos dados.\n\n\n\n",
    "preview": "posts/2021-01-03-joins-no-r-e-no-postgresql/inner_join.png",
    "last_modified": "2021-01-03T14:07:27-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-pivotagem-com-r-e-postgresql/",
    "title": "Pivotagem com R e PostgreSQL",
    "description": "Nesta postagem, eu mostro como converter dataframes e tabelas\nentre os formatos wide e long tanto no R quanto no PostgreSQL,\nalém de permitir realizar essas operações no banco a partir do R.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br/pivotagem"
      }
    ],
    "date": "2021-01-03",
    "categories": [],
    "contents": "\nPivotagem\nNeste tutorial, mostraremos como pivotar no R e no PostgreSQL. Pivotar, como o próprio nome sugere, significa rotacionar colunas ou linhas, de modo que, valores contidos em linhas, passam a ser colunas e vice-versa. Em ciência de dados, é mais comum falarmos em formatos wide e formato long. Em SQL, é mais comum falarmos em pivoting e unpivoting, respectivamente. A imagem abaixo ilustra o processo de pivotatem:\n\n\n\nEste tutorial cobre as situações mais simples de pivotagem. No dia a dia da cientista de dados ou da DBA, dataframes e tabelas apresentam-se como estruturas um tanto complexas, as quais exigem transformações prévias para não haver perda de dados ou mesmo resultados indesejados.\nPara começar, vamos importar um simples dataframe em formato wide com os ids dos clientes e pagamentos realizados em cada mês no ano. Se você já tem um banco PostgreSQL rodando, aproveite para escrever esse mesmo dataframe no banco. O dataframe encontra-se aqui\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n\n\ncliente\njan\nfev\nmar\nabr\nmai\njun\njul\nago\nset\nout\nnov\ndez\n1\n790\n205\n724\n724\n388\n490\n112\n483\n230\n385\n877\n317\n2\n612\n513\n281\n360\n839\n727\n953\n956\n133\n763\n192\n490\n3\n250\n414\n142\n460\n555\n223\n213\n240\n410\n502\n955\n543\n4\n741\n994\n365\n113\n974\n525\n875\n445\n908\n845\n444\n370\n5\n117\n101\n395\n335\n612\n765\n163\n839\n516\n234\n361\n614\n6\n828\n139\n137\n943\n552\n767\n654\n858\n379\n823\n489\n269\n7\n722\n281\n380\n594\n727\n617\n612\n344\n441\n731\n968\n201\n8\n606\n968\n749\n627\n336\n938\n159\n466\n319\n867\n733\n896\n9\n629\n338\n525\n978\n557\n158\n827\n871\n645\n356\n797\n953\n10\n955\n174\n644\n602\n749\n110\n171\n870\n692\n757\n882\n570\n\nPerceba que os dados estão em formato wide. Cada mês ocupa uma coluna.\nMotivação\nPivotagem é ubiqua tanto em ciência de dados quanto em SQL. Nos grupos do Telegram R Brasil e PostgreSQL BR, questões sobre pivotagem são bastante frequentes.\nNo R, há muitos pacotes e funções destinados à conversão de wide para long e vice-versa. No PostgreSQL, conheço apenas uma extensão, tablefunc, voltada à pivoting, ou seja, colocação da tabela na formato wide. Desconheço qualquer extensão ou função oficial para conversão de wide para long. Por outro lado, a comunidade tem oferecido várias soluções para esta operação. Este tutorial é tributário das sugestões oferecidas por Matheus Olivieira no grupo de PostgreSQL do Telegram e desta solução no SO.\nPivotagem consiste numa ferramenta poderosa para solução de problemas concretos com dados e sua compreensão é uma exigência básica para quem quer manusear dados eficientemente. Alguns exemplos que me vieram à mente enquanto escrevia este tutorial são:\nFormato wide é quase sempre mais apropriado para disponibilizar os dados para visualização. É muito mais fácil observar dados de pagamentos de clientes mês a mês, tendo cada cliente numa linha e cada um dos meses numa coluna.\nFormato wide é também a forma natural como os dados são preenchidos num formulário ou mesmo numa planilha.\nFormato long é, por sua vez, muito melhor para proceder a análises e plotar gráficos. Ao colocar todos os meses numa única coluna e seus respectivos valores em outra, mantendo um terceira com o id ou nome do cliente, você só tem de informar que no eixo x vão os meses e no eixo y vão os valores.\nQuando se realiza webscraping, frequentemente ocorre de não sabermos de antemão quais e quantas variáveis virão em cada página. A forma mais segura de realizar webscraping é manter todas a variáveis numa única coluna com seus respectivos valores em outra, para, num segundo momento, caso seja necessário, converter para formato wide. Esta opção é particularmente útil quando estamos realizando webscraping com R e enviando os dataframes já parseados para o PostgreSQL. Se convertermos para o formato wide de pronto, corremos o risco de tentar inserir uma coluna não existente na tabela contida no banco. Em formato long, esse risco não existe.\nPivotagem no R\nDe wide para long\nComo o dataframe importado acima está em formato wide, vamos colocá-lo em formato long. Para tanto, utilizaremos o pacote tidyr, que já está carregado juntamente com os demais pacotes do tidyverse. Como queremos colocar todos os meses numa mesma coluna e seus respectivos valores numa segunda coluna, é suficiente chamar a função pivot_longer, informando que as colunas são o complemento da coluna cliente.\n\n\npivot_longer(pagamentos,!cliente)\n\n\n# A tibble: 120 x 3\n   cliente name  value\n     <int> <chr> <int>\n 1       1 jan     790\n 2       1 fev     205\n 3       1 mar     724\n 4       1 abr     724\n 5       1 mai     388\n 6       1 jun     490\n 7       1 jul     112\n 8       1 ago     483\n 9       1 set     230\n10       1 out     385\n# … with 110 more rows\n\nSe você quiser informar os novos nomes das variáveis, melhor:\n\n\npagamentos_long <- pivot_longer(pagamentos, \n                                !cliente, \n                                names_to = \"mes\", \n                                values_to =\"valor_pago\")\n\n\n\n\n# A tibble: 120 x 3\n   cliente mes   valor_pago\n     <int> <chr>      <int>\n 1       1 jan          790\n 2       1 fev          205\n 3       1 mar          724\n 4       1 abr          724\n 5       1 mai          388\n 6       1 jun          490\n 7       1 jul          112\n 8       1 ago          483\n 9       1 set          230\n10       1 out          385\n# … with 110 more rows\n\nVeja como fica fácil plotar um gráfico deste novo dataframe:\n\n\npagamentos_long %>% \n  mutate(mes = factor(mes, levels = unique(mes))) %>% \n  ggplot(aes(x = mes, y = valor_pago, fill = mes))+\n  geom_bar(stat = \"identity\") +\n  theme_classic()\n\n\n\n\nDe long para wide\nSe quisermos converter pagamentos_long para pagamentos_wide, ou seja, retornar o formato inicial, basta usar a função pivot_wider. Para tanto, você deve informar quais a colunas ids, se não informar, o R irá usar todas as colunas, com exceção da coluna cujos valores se converterão em colunas, especificada no argumento names_from, e da coluna com valores que serão alocados para as respectivas novas colunas, respectivamente, arguemento values_from.\n\n\npagamentos_wide <- pivot_wider(pagamentos_long, \n                               id_cols = cliente, \n                               names_from = mes, \n                               values_from = valor_pago)\n\n\n\n\ncliente\njan\nfev\nmar\nabr\nmai\njun\njul\nago\nset\nout\nnov\ndez\n1\n790\n205\n724\n724\n388\n490\n112\n483\n230\n385\n877\n317\n2\n612\n513\n281\n360\n839\n727\n953\n956\n133\n763\n192\n490\n3\n250\n414\n142\n460\n555\n223\n213\n240\n410\n502\n955\n543\n4\n741\n994\n365\n113\n974\n525\n875\n445\n908\n845\n444\n370\n5\n117\n101\n395\n335\n612\n765\n163\n839\n516\n234\n361\n614\n6\n828\n139\n137\n943\n552\n767\n654\n858\n379\n823\n489\n269\n7\n722\n281\n380\n594\n727\n617\n612\n344\n441\n731\n968\n201\n8\n606\n968\n749\n627\n336\n938\n159\n466\n319\n867\n733\n896\n9\n629\n338\n525\n978\n557\n158\n827\n871\n645\n356\n797\n953\n10\n955\n174\n644\n602\n749\n110\n171\n870\n692\n757\n882\n570\n\nPivotagem no PostgreSQL\nDe wide para long\nA conversão para formato long no PostgreSQL pode ser feita de várias formas. Eu particularmente gosto da solução apresentada à uma questão no StackOverflow, que basicamente converte as linhas em arrays e, em seguida, dá um unnest em cada um desses arrays. Veja abaixo:\n\nSELECT cliente,\n       unnest(array['jan', 'fev', 'mar','abr','mai','jun','jul','ago','set','out','nov','dez']) AS mes,\n       unnest(array[jan, fev, mar,abr,mai,jun,jul,ago,set,out, nov, dez ]) AS valor_pago\nFROM pagamentos\nORDER BY cliente;\nA limitação deste query é que você tem de mencionar todas as variáveis duas vezes. Para facilitar sua vida, eu criei uma função que usa tidyselect\": pg_pivot_longer. Deste modo, é suficiente chamá-la:\npagamento_long <- pg_pivot_longer(conn,\n                                  tbl = \"pagamentos\",\n                                  cols = !cliente, \n                                  names_to = \"mes\", \n                                  values_to = \"valor_pago\")\nOu se quiser criar no próprio banco, sem importar para o R:\npg_pivot_longer(conn, \n               tbl=\"pagamentos\", \n               new_tbl= \"pagamentos_long\",\n               cols = !cliente, \n               names_to = \"mes\", \n               values_to = \"valor_pago\")\nDe long para wide\nPara converter para formato wide, há muitas formas e você pode consultá-las nesse excelente blog post\nNeste tutorial, irei focar naquela que me parece ser a mais simples e popular, que é usar a cláusula FILTER do PostgreSQL. Vejamos como fica:\n\nSELECT cliente,\n       sum(valor_pago) FILTER (WHERE mes = 'jan') as jan,\n       sum(valor_pago) FILTER (WHERE mes = 'fev') as fev,\n       sum(valor_pago) FILTER (WHERE mes = 'mar') as mar,\n       sum(valor_pago) FILTER (WHERE mes = 'abr') as abr,\n       sum(valor_pago) FILTER (WHERE mes = 'mai') as mai,\n       sum(valor_pago) FILTER (WHERE mes = 'jun') as jun,\n       sum(valor_pago) FILTER (WHERE mes = 'jul') as jul,\n       sum(valor_pago) FILTER (WHERE mes = 'ago') as ago,\n       sum(valor_pago) FILTER (WHERE mes = 'set') as set,\n       sum(valor_pago) FILTER (WHERE mes = 'out') as out,\n       sum(valor_pago) FILTER (WHERE mes = 'nov') as nov,\n       sum(valor_pago) FILTER (WHERE mes = 'dez') as dez\nfrom pagamentos_long\ngroup by cliente\norder by cliente;\nComo na transformação para formato longo, aqui você tem de escrever uma linha para cada uma das novas variáveis. Para facilitar sua vida, eu criei uma função chamada pg_pivot_wider, que permite simplesmente informar qual a coluna onde se encontram as variáveis e qual a coluna onde se encontram os valores, além de informar a as colunas com os ids.\npagamentos_wide <- pg_pivot_wider(conn,\n                                tbl = \"pagamentos_long\", \n                                id_cols= \"cliente\", \n                                names_from = \"mes\", \n                                values_from = \"valor_pago\")\nComo acima, se quiser criar uma nova tabela no banco, basta oferecer o novo nome ao argumento new_tbl\npg_pivot_wider(conn,\n               tbl = \"pagamentos_long\",\n               new_tbl = \"pagamentos_wide\", \n               id_cols= \"cliente\",\n               names_from = \"mes\", \n               values_from = \"valor_pago\")\nAs funções pg_pivot_longer e pg_pivot_wider estão em fase experimental. Não se recomenda usá-las em produção.\n\n\n\n",
    "preview": "posts/2021-01-03-pivotagem-com-r-e-postgresql/pivotagem2.png",
    "last_modified": "2021-01-03T12:30:12-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-similaridades-entre-na-no-r-e-null-no-postgresql/",
    "title": "Similaridades entre NA no R e NULL no PostgreSQL",
    "description": "Neste tuturial iremos mostrar como o comportamento de `NAs` no **R** \né consistente com o de `NULL` no PostgreSQL.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-03",
    "categories": [],
    "contents": "\nIntrodução\nQuando você envia um dataframe do R para PostgreSQL, NAs, ou seja, valores faltantes, são convertidos para NULL. Acontece que o R diferencia NA de NULL. No R NULL é tipo de objeto e é por isso que você não consegue incluí-lo num vetor ou numa matriz, mas consegue adicioná-lo como elemento numa lista, inclusive em dataframes, já que estas também são listas. Porém, somente quando todas as colunas são NULL. NULL no R é um tanto ambíguo, pois ora se comporta como vazio, ora como elemento indefinido.\nPor sua vez, NA no R é um elemento lógico para indicar dado desconhecido. Isso é concebido dentro da lógica ternária, que admite três valores: VERDADEIRO, FALSO e DESCONHECIDO. O mesmo acontece com SQL para NULL.\nNeste tuturial iremos mostrar como o comportamento de NAs no R é consistente com o de NULL no PostgreSQL.\nCriando objetos com valores desconhecidos\nPara criar objetos com valores desconhecidos no R, você simplesmente coloca NA no vetor ou na coluna:\n\n\na <- c(1,2,3,NA)\n\na\n\n\n[1]  1  2  3 NA\n\n\n\ndf <- data.frame(a = a)\ndf\n\n\n   a\n1  1\n2  2\n3  3\n4 NA\n\nNo PostgreSQL, você pode fazer o mesmo, mas com NULL\n\nSELECT  UNNEST(ARRAY[1,2,3,NULL]) AS a;\na\n1\n2\n3\n(4 rows)\n\nCREATE TABLE df (a) AS VALUES (1),(2),(3), (NULL);\n\nSELECT * FROM df;\na\n1\n2\n3\n(4 rows)\nSimilaridades\nComo ambos são valores desconhecidos, espera-se que o comportamento seja similar num e noutro, e de fato é, em muitas situações.\nComparações no R\n\n\n2 == NA\n\n\n[1] NA\n\n\n\nNA == NA\n\n\n[1] NA\n\n\n\n2 < NA\n\n\n[1] NA\n\n\n\n2 + NA\n\n\n[1] NA\n\nEm todas essas situações, o R retorna o elemento lógico NA, ou seja, desconhecido, porque o resultado da comparação é de fato desconhecido, já que NA pode equivaler a uma infinidade de valores, inclusive o valor comparado a NA.\nComo NULL possui a mesma natureza, as respostas são consistentes:\nSELECT 2 = NULL;\n?column?\n(1 row)\n\nSELECT NULL = NULL;\n?column?\n(1 row)\nSELECT 2 < NULL;\n?column?\n(1 row)\nSELECT 2 + NULL;\n?column?\n(1 row)\nIgualmente, algo que surpreende no R é o filtro negativo em colunas que contêm NAs:\n\n\nsubset(df,a !=2)\n\n\n  a\n1 1\n3 3\n\nCom dplyr ocorre o mesmo:\n\n\ndplyr::filter(df,a != 2)\n\n\n  a\n1 1\n2 3\n\nEm ambos os casos, linhas com NAs também foram excluídas. Isso porque NA pode conter o 2 (Confesso que esse comportamento não me convence inteiramente).\nVejamos em SQL\nSELECT * FROM df\nWHERE  a != 2;\na\n1\n3\n(2 rows)\nPara evitar surpresas, não se esqueça de explicitamente filtrar para NAs no R e para NULLs no PostgreSQL.\n\n\ndplyr::filter(df, a != 2 | is.na(a))\n\n\n   a\n1  1\n2  3\n3 NA\n\nSELECT * \nFROM df\nWHERE a != 2\nOR a IS NULL;\na\n1\n3\n(3 rows)\nConclusão\nNão estou absolutamente seguro de que os comportamentos de NA no R e NULL em SQL são sempre os mesmos, mas para as operações mais comuns, você pode ficar tranquila que são.\n\n\n\n",
    "preview": "posts/2021-01-03-similaridades-entre-na-no-r-e-null-no-postgresql/missing.jpg",
    "last_modified": "2021-01-03T16:43:45-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-tidypg-cheatsheet-criando-tabelas-a-partir-de-outras-parte-1/",
    "title": "TidyPG cheatsheet: Criando tabelas a partir de outras - parte 1",
    "description": "Neste cheatsheet mostramos como criar tabelas tanto no R \nquanto no PostgreSQL a partir da junção de outras ou de filtros.",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-03",
    "categories": [],
    "contents": "\nSelecionando linhas e colunas\nSelecionando colunas\n\n\nTidyverse\n## Seleção simples\ndf <- df1 %>% \n      select(coluna1,coluna2,...)\n\n## Selecionar e renomear\ndf <- df %>% \n      select(col1 = coluna1, \n             col2 = coluna2,...)\n\n\n\n\n\nPostgreSQL\n-- Seleção simples\nCREATE TABLE df AS \n(SELECT coluna1, coluna2,... FROM df1); \n\n-- Selecionar e renomear \nCREATE TABLE df AS\n(SELECT coluna1 col1, coluna2 col2,... FROM df1);\n\n\nSelecionando linhas - slice(dplyr)/limit-offset(PostgreSQL)\n\n\nTidyverse\n## Filtrar as 10 primeiras linhas\ndf <- df1 %>% \n      slice(1:10) \n## Filtrar as linhas de 3 a 6\ndf <- df %>% \n      slice(3:6) \n\n\n\n\n\nPostgreSQL\nBases relacionais não possuem uma noção intrínseca de ordem. Especifique uma coluna para ordenar, antes de filtrar.\n\nALTER TABLE df1 ADD COLUMN df1_id SERIAL;\n\nCREATE TABLE df AS \n(SELECT * \n FROM df1\n ORDER BY  df1_id\n LIMIT 10);\n\n-- Eventualmente apague esta coluna\n\nALTER TABLE df DROP COLUMN df1_id;\n\nCREATE TABLE df AS\n(SELECT *\nFROM df1\nORDER BY df1_id\nLIMIT 4\nOFFSET 2);\n\n-- Eventualmente apague esta coluna\nALTER TABLE df DROP COLUMN df1_id; \n\n\nSelecionando linhas - filter(dplyr)/where(PostgreSQL)\n\n\nTidyverse\ndf <- df1 %>% \n      filter(col1 == valor)\n      \ndf <- df1 %>% \n      filter(col1 != valor)\n      \ndf <- df1 %>% \n      filter(col1 > valor)\n      \ndf <- df1 %>% \n      filter(col1 < valor)\n      \ndf <- df1 %>% \n      filter(col1 >= valor)\n      \ndf <- df1 %>% \n      filter(col1 <= valor)\n\n\n\n\n\nPostgreSQL\n\nCREATE TABLE df AS \n(SELECT * \n FROM df1\n WHERE col1 = valor);\n\nCREATE TABLE df AS \n(SELECT * \n FROM df1\n WHERE col1 != valor);\n\nCREATE TABLE df AS \n(SELECT * \n FROM df1\n WHERE col1 > valor);\n\nCREATE TABLE df AS \n(SELECT * \n FROM df1\n WHERE col1 < valor);\n \nCREATE TABLE df AS \n(SELECT * \n FROM df1\n WHERE col1 >= valor); \n \nCREATE TABLE df AS \n(SELECT * \n FROM df1\n WHERE col1 <= valor);\n\n\nJuntando tabelas/dataframes\nJunção vertical\n\n\nTidyverse\n## preserva linhas duplicadas\ndf <- bind_rows(df1,df2) \n\n## elimina dupliações\ndf <- bind_rows(df1,df2) %>% \n      distinct()  \n\n\n\n\n\nPostgreSQL\n-- preserva duplicações\nCREATE TABLE df AS \n(SELECT * FROM df1 \nUNION ALL          \nSELECT * FROM df2);\n\n-- elimina duplicações\nCREATE TABLE df AS \n(SELECT * FROM df1 \nUNION           \nSELECT * FROM df2);\n\n\nJunção horizontal\n\n\nTidyverse\ndf <- bind_cols(df1,df2)\n\n\n\n\n\nPostgreSQL\nHá formas mais eficientes de realizar esta combinação, mas falaremos disso quando tratarmos de junções (joins)\n\nALTER TABLE df1 ADD COLUMN df1_id serial;\nALTER TABLE df2 ADD COLUMN df2_id serial;\n\nCREATE TABLE df as \n(SELECT * FROM df1,df2\nWHERE df1.df1_id = df2.df2_id);\n\nALTER TABLE df DROP COLUMN df2_id;\nALTER TABLE df RENAME COLUMN df1_id to df_id;\n\n\nProduto cartesiano ( cross, expand.grid)\n\n\nTidyverse/R\n## R base\ndf <- expand.grid(list(v1=1:2,\n              v2=c(\"a\",\"b\",\"c\"))) \n## purrr\ndf <- purrr::cross_df(list(v1=1:2,\n              v2=c(\"a\",\"b\",\"c\")))\n\n\n\n\n\nPostgreSQL\nCREATE TABLE  df\nAS\n(SELECT v1,v2 FROM df1, df2);\n\n\n\n\n\n",
    "preview": "posts/2021-01-03-tidypg-cheatsheet-criando-tabelas-a-partir-de-outras-parte-1/bind_rows.png",
    "last_modified": "2021-01-03T17:05:03-03:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Bem-vinda à  Ciência de Dados com R e PostgreSQL",
    "description": "Bem-vindo a este blog sobre Ciência de Dados com R e PostgreSQL. Periodicamente, irei\npostar tutoriais sobre !",
    "author": [
      {
        "name": "José de Jesus Filho",
        "url": "https://rpg.consudata.com.br"
      }
    ],
    "date": "2021-01-03",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-03T09:50:45-03:00",
    "input_file": {}
  }
]
